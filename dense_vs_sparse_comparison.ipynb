{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense vs Sparse Attention Comparison\n",
    "\n",
    "## Complete Training and Benchmarking Pipeline\n",
    "\n",
    "This notebook:\n",
    "1. Trains a **Dense Attention** Pythia-70M model\n",
    "2. Trains a **Sparse Attention** Pythia-70M model  \n",
    "3. Compares **training speed**, **inference speed**, and **model quality**\n",
    "4. Visualizes all results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GPU availability...\n",
      "✓ GPU Available\n",
      "\n",
      "Rebuilding project...\n",
      "-- Found pybind11: /usr/local/lib/python3.12/dist-packages/pybind11/include (found version \"3.0.1\")\n",
      "-- Found cuda, building cuda backend\n",
      "Sat Nov 22 23:27:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:42:00.0 Off |                    0 |\n",
      "| N/A   27C    P0            112W /  500W |    1059MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.0\n",
      "-- Configuring done (0.5s)\n",
      "-- Generating done (0.2s)\n",
      "-- Build files have been written to: /workspace/manav/dl_sys_project/build\n",
      "make[1]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "make[2]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "make[3]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "make[3]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "make[3]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "make[3]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX shared module /workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray_backend_cuda.cpython-312-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "make[1]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "Using needle backend\n",
      "\n",
      "✓ All imports successful!\n",
      "Needle backend: nd\n",
      "✓ Using CUDA (GPU)\n",
      "\n",
      "================================================================================\n",
      "SETUP COMPLETE - READY TO TRAIN!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Set project path - ADJUST THIS TO YOUR PATH\n",
    "project_path = '/workspace/manav/dl_sys_project/'\n",
    "os.chdir(project_path)\n",
    "\n",
    "# Check GPU\n",
    "print(\"Checking GPU availability...\")\n",
    "try:\n",
    "    import subprocess\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT)\n",
    "    print(\"✓ GPU Available\")\n",
    "except:\n",
    "    print(\"✗ No GPU detected - will use CPU\")\n",
    "\n",
    "# Rebuild project\n",
    "print(\"\\nRebuilding project...\")\n",
    "# !make clean\n",
    "!make\n",
    "\n",
    "# Setup paths\n",
    "sys.path.insert(0, os.path.join(project_path, 'python'))\n",
    "sys.path.insert(0, os.path.join(project_path, 'apps'))\n",
    "\n",
    "# Imports\n",
    "import needle as ndl\n",
    "import needle.nn as nn\n",
    "from pythia_model import create_pythia_70m, PythiaConfig\n",
    "from train_pythia import (\n",
    "    train, \n",
    "    load_dataset_huggingface, \n",
    "    load_synthetic_data,\n",
    "    save_checkpoint, \n",
    "    load_checkpoint,\n",
    "    evaluate\n",
    ")\n",
    "from needle.nn.nn_sparse_attention import BlockSparsePattern\n",
    "\n",
    "print(\"\\n✓ All imports successful!\")\n",
    "print(f\"Needle backend: {ndl.backend_selection.BACKEND}\")\n",
    "\n",
    "# Set device\n",
    "try:\n",
    "    device = ndl.cuda()\n",
    "    print(\"✓ Using CUDA (GPU)\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ CUDA not available: {e}\")\n",
    "    device = ndl.cpu()\n",
    "    print(\"Using CPU instead\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SETUP COMPLETE - READY TO TRAIN!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATASET\n",
      "================================================================================\n",
      "\n",
      "Attempting to load WikiText-2...\n",
      "Loading wikitext-2 from HuggingFace...\n",
      "Tokenizing...\n",
      "Vocabulary size: 66653\n",
      "Train tokens: 100000\n",
      "Validation tokens: 10000\n",
      "\n",
      "================================================================================\n",
      "DATASET SUMMARY\n",
      "================================================================================\n",
      "Dataset: WikiText-2\n",
      "Vocabulary size: 66,653\n",
      "Train tokens: 100,000\n",
      "Validation tokens: 10,000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try to load WikiText-2, fallback to synthetic\n",
    "try:\n",
    "    print(\"\\nAttempting to load WikiText-2...\")\n",
    "    train_data, val_data, vocab_size = load_dataset_huggingface(\n",
    "        \"wikitext-2\", \n",
    "        max_tokens=100000  # Use 100k tokens for faster training\n",
    "    )\n",
    "    dataset_name = \"WikiText-2\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nCouldn't load WikiText-2: {e}\")\n",
    "    print(\"Using synthetic data instead...\")\n",
    "    train_data, val_data, vocab_size = load_synthetic_data(100000)\n",
    "    dataset_name = \"Synthetic\"\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Train tokens: {len(train_data):,}\")\n",
    "print(f\"Validation tokens: {len(val_data):,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING CONFIGURATION\n",
      "================================================================================\n",
      "Epochs: 10\n",
      "Batch size: 8\n",
      "Sequence length: 128\n",
      "Learning rate: 0.0003\n",
      "Device: cuda()\n",
      "Dataset: WikiText-2\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 10  # Increase for better results\n",
    "BATCH_SIZE = 8\n",
    "SEQ_LEN = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Sequence length: {SEQ_LEN}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Dense Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING DENSE ATTENTION MODEL\n",
      "================================================================================\n",
      "\n",
      "Creating dense model...\n",
      "Created Pythia-70M model with ~53.1M parameters\n",
      "Sparse attention: False\n",
      "\n",
      "Model: Pythia-70M (Dense Attention)\n",
      "Parameters: ~53.1M\n",
      "\n",
      "Starting training at 23:27:52...\n",
      "\n",
      "================================================================================\n",
      "Training Configuration\n",
      "================================================================================\n",
      "Model: Pythia-70M\n",
      "Sparse attention: False\n",
      "Epochs: 10\n",
      "Batch size: 8\n",
      "Sequence length: 128\n",
      "Learning rate: 0.0003\n",
      "Device: cuda()\n",
      "Eval only: False\n",
      "================================================================================\n",
      "Preparing data...\n",
      "Train batches: 96\n",
      "Val batches: 9\n",
      "\n",
      "================================================================================\n",
      "Epoch 1/10\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m start_time_dense = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m results_dense = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dense\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_dense\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./checkpoints/dense\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m train_time_dense = time.time() - start_time_dense\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/apps/train_pythia.py:412\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_data, val_data, config, n_epochs, batch_size, seq_len, lr, device, checkpoint_dir, eval_only)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m train_loss, tokens_per_sec = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m train_losses.append(train_loss)\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/apps/train_pythia.py:158\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, batches, optimizer, device, clip_grad)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    157\u001b[39m optimizer.reset_grad()\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m    161\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_basic.py:74\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/apps/pythia_model.py:177\u001b[39m, in \u001b[36mPythiaLM.forward\u001b[39m\u001b[34m(self, input_ids, targets)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Apply transformer layers\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Final layer norm\u001b[39;00m\n\u001b[32m    180\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_norm(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_basic.py:74\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_transformer.py:341\u001b[39m, in \u001b[36mTransformerLayer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    337\u001b[39m batch_size, seq_len, x_dim = x.shape\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m### BEGIN YOUR SOLUTION\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;66;03m# First residual block: x = x + Dropout(Attention(x))\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m attn_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Self-attention\u001b[39;00m\n\u001b[32m    342\u001b[39m attn_out = \u001b[38;5;28mself\u001b[39m.attn_dropout(attn_out)\n\u001b[32m    343\u001b[39m x = x + attn_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_basic.py:74\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_transformer.py:258\u001b[39m, in \u001b[36mAttentionLayer.forward\u001b[39m\u001b[34m(self, q, k, v)\u001b[39m\n\u001b[32m    255\u001b[39m v_heads = ops.transpose(v_heads, axes=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# Step 4: Apply multi-head attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m attn_output, probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_heads\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, num_head, queries_len, dim_head)\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Step 5: Transpose back (B, num_head, queries_len, dim_head) -> (B, queries_len, num_head, dim_head)\u001b[39;00m\n\u001b[32m    261\u001b[39m attn_output = ops.transpose(attn_output, axes=(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_basic.py:74\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_transformer.py:143\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, q, k, v)\u001b[39m\n\u001b[32m    141\u001b[39m v_transpose = ops.transpose(v, axes=(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m))\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Compute probs @ v: (B, H, T, T) @ (B, H, T, D) -> (B, H, T, D)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_transpose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m### END YOUR SOLUTION\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result, probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_transformer.py:69\u001b[39m, in \u001b[36mMultiHeadAttention.matmul\u001b[39m\u001b[34m(self, a, b_transpose)\u001b[39m\n\u001b[32m     66\u001b[39m broadcast_shape[-\u001b[32m3\u001b[39m] = a_shape[-\u001b[32m3\u001b[39m]\n\u001b[32m     67\u001b[39m b_transpose = b_transpose.broadcast_to(broadcast_shape)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_transpose\u001b[49m).sum(\u001b[38;5;28mlen\u001b[39m(a.shape) - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:319\u001b[39m, in \u001b[36mTensor.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mneedle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEWiseMul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m needle.ops.MulScalar(other)(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:80\u001b[39m, in \u001b[36mTensorOp.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_from_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:242\u001b[39m, in \u001b[36mTensor.make_from_op\u001b[39m\u001b[34m(op, inputs)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor.requires_grad:\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor.detach()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealize_cached_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:107\u001b[39m, in \u001b[36mValue.realize_cached_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cached_data\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# note: data implicitly calls realized cached data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28mself\u001b[39m.cached_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealize_cached_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cached_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/ops/ops_mathematic.py:73\u001b[39m, in \u001b[36mEWiseMul.compute\u001b[39m\u001b[34m(self, a, b)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m b.shape != broadcast_shape:\n\u001b[32m     71\u001b[39m         b = b.broadcast_to(broadcast_shape)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:543\u001b[39m, in \u001b[36mNDArray.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: Union[\u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mewise_or_scalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mewise_mul\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscalar_mul\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:524\u001b[39m, in \u001b[36mNDArray.ewise_or_scalar\u001b[39m\u001b[34m(self, other, ewise_func, scalar_func)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, NDArray):\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shape == other.shape, \u001b[33m\"\u001b[39m\u001b[33moperation needs two equal-sized arrays\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     ewise_func(\u001b[38;5;28mself\u001b[39m.compact()._handle, \u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m._handle, out._handle)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    526\u001b[39m     scalar_func(\u001b[38;5;28mself\u001b[39m.compact()._handle, other, out._handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:230\u001b[39m, in \u001b[36mNDArray.compact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     out = \u001b[43mNDArray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28mself\u001b[39m.device.compact(\n\u001b[32m    232\u001b[39m         \u001b[38;5;28mself\u001b[39m._handle, out._handle, \u001b[38;5;28mself\u001b[39m.shape, \u001b[38;5;28mself\u001b[39m.strides, \u001b[38;5;28mself\u001b[39m._offset\n\u001b[32m    233\u001b[39m     )\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:161\u001b[39m, in \u001b[36mNDArray.make\u001b[39m\u001b[34m(shape, strides, device, handle, offset)\u001b[39m\n\u001b[32m    159\u001b[39m array._device = device \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_device()\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     array._handle = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    163\u001b[39m     array._handle = handle\n",
      "\u001b[31mRuntimeError\u001b[39m: out of memory"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DENSE ATTENTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create dense model\n",
    "print(\"\\nCreating dense model...\")\n",
    "model_dense, config_dense = create_pythia_70m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=SEQ_LEN,\n",
    "    use_sparse_attention=False,  # DENSE\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nModel: Pythia-70M (Dense Attention)\")\n",
    "print(f\"Parameters: ~{config_dense.get_total_params() / 1e6:.1f}M\")\n",
    "\n",
    "# Train\n",
    "print(f\"\\nStarting training at {datetime.now().strftime('%H:%M:%S')}...\\n\")\n",
    "start_time_dense = time.time()\n",
    "\n",
    "results_dense = train(\n",
    "    model=model_dense,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    config=config_dense,\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seq_len=SEQ_LEN,\n",
    "    lr=LEARNING_RATE,\n",
    "    device=device,\n",
    "    checkpoint_dir='./checkpoints/dense'\n",
    ")\n",
    "\n",
    "train_time_dense = time.time() - start_time_dense\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"DENSE MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training time: {train_time_dense/60:.2f} minutes\")\n",
    "print(f\"Time per epoch: {train_time_dense/EPOCHS:.2f} seconds\")\n",
    "print(f\"Final train loss: {results_dense['train_losses'][-1]:.4f}\")\n",
    "print(f\"Final val loss: {results_dense['val_losses'][-1]:.4f}\")\n",
    "print(f\"Best val loss: {results_dense['best_val_loss']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Sparse Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING SPARSE ATTENTION MODEL\n",
      "================================================================================\n",
      "\n",
      "Creating sparse model...\n",
      "Created Pythia-70M model with ~53.1M parameters\n",
      "Sparse attention: True\n",
      "\n",
      "Model: Pythia-70M (Sparse Attention)\n",
      "Parameters: ~53.1M\n",
      "Sparse pattern: local\n",
      "Block size: 64\n",
      "\n",
      "Starting training at 23:26:52...\n",
      "\n",
      "================================================================================\n",
      "Training Configuration\n",
      "================================================================================\n",
      "Model: Pythia-70M\n",
      "Sparse attention: True\n",
      "Epochs: 10\n",
      "Batch size: 16\n",
      "Sequence length: 128\n",
      "Learning rate: 0.0003\n",
      "Device: cuda()\n",
      "Eval only: False\n",
      "================================================================================\n",
      "Preparing data...\n",
      "Train batches: 48\n",
      "Val batches: 4\n",
      "\n",
      "================================================================================\n",
      "Epoch 1/10\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m start_time_sparse = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m results_sparse = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./checkpoints/sparse\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m train_time_sparse = time.time() - start_time_sparse\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/apps/train_pythia.py:412\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_data, val_data, config, n_epochs, batch_size, seq_len, lr, device, checkpoint_dir, eval_only)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    411\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m train_loss, tokens_per_sec = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m train_losses.append(train_loss)\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/apps/train_pythia.py:161\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, batches, optimizer, device, clip_grad)\u001b[39m\n\u001b[32m    158\u001b[39m logits, loss = model(inputs, targets)\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Gradient clipping (optional)\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clip_grad > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:297\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, out_grad)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, out_grad=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    292\u001b[39m     out_grad = (\n\u001b[32m    293\u001b[39m         out_grad\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m out_grad\n\u001b[32m    295\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m init.ones(*\u001b[38;5;28mself\u001b[39m.shape, dtype=\u001b[38;5;28mself\u001b[39m.dtype, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    296\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[43mcompute_gradient_of_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:403\u001b[39m, in \u001b[36mcompute_gradient_of_variables\u001b[39m\u001b[34m(output_tensor, out_grad)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# If this node has an operation, propagate gradients to inputs\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node.op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     input_grads = \u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient_as_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, input_node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(node.inputs):\n\u001b[32m    405\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m input_node \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m node_to_output_grads_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:67\u001b[39m, in \u001b[36mOp.gradient_as_tuple\u001b[39m\u001b[34m(self, out_grad, node)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgradient_as_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, out_grad: \u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m, node: \u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m) -> Tuple[\u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     66\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convenience method to always return a tuple from gradient call\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m     69\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/ops/ops_mathematic.py:79\u001b[39m, in \u001b[36mEWiseMul.gradient\u001b[39m\u001b[34m(self, out_grad, node)\u001b[39m\n\u001b[32m     76\u001b[39m lhs, rhs = node.inputs\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m grad_lhs = \u001b[43mout_grad\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\n\u001b[32m     80\u001b[39m grad_rhs = out_grad * lhs\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Handle broadcasting in gradients\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:319\u001b[39m, in \u001b[36mTensor.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mneedle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEWiseMul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m needle.ops.MulScalar(other)(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:80\u001b[39m, in \u001b[36mTensorOp.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_from_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:242\u001b[39m, in \u001b[36mTensor.make_from_op\u001b[39m\u001b[34m(op, inputs)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor.requires_grad:\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor.detach()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealize_cached_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:107\u001b[39m, in \u001b[36mValue.realize_cached_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cached_data\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# note: data implicitly calls realized cached data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28mself\u001b[39m.cached_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealize_cached_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cached_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/ops/ops_mathematic.py:73\u001b[39m, in \u001b[36mEWiseMul.compute\u001b[39m\u001b[34m(self, a, b)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m b.shape != broadcast_shape:\n\u001b[32m     71\u001b[39m         b = b.broadcast_to(broadcast_shape)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:543\u001b[39m, in \u001b[36mNDArray.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: Union[\u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mewise_or_scalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mewise_mul\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscalar_mul\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:524\u001b[39m, in \u001b[36mNDArray.ewise_or_scalar\u001b[39m\u001b[34m(self, other, ewise_func, scalar_func)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, NDArray):\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shape == other.shape, \u001b[33m\"\u001b[39m\u001b[33moperation needs two equal-sized arrays\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     ewise_func(\u001b[38;5;28mself\u001b[39m.compact()._handle, \u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m._handle, out._handle)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    526\u001b[39m     scalar_func(\u001b[38;5;28mself\u001b[39m.compact()._handle, other, out._handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:230\u001b[39m, in \u001b[36mNDArray.compact\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     out = \u001b[43mNDArray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28mself\u001b[39m.device.compact(\n\u001b[32m    232\u001b[39m         \u001b[38;5;28mself\u001b[39m._handle, out._handle, \u001b[38;5;28mself\u001b[39m.shape, \u001b[38;5;28mself\u001b[39m.strides, \u001b[38;5;28mself\u001b[39m._offset\n\u001b[32m    233\u001b[39m     )\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:161\u001b[39m, in \u001b[36mNDArray.make\u001b[39m\u001b[34m(shape, strides, device, handle, offset)\u001b[39m\n\u001b[32m    159\u001b[39m array._device = device \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_device()\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     array._handle = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    163\u001b[39m     array._handle = handle\n",
      "\u001b[31mRuntimeError\u001b[39m: out of memory"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SPARSE ATTENTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create sparse model\n",
    "print(\"\\nCreating sparse model...\")\n",
    "model_sparse, config_sparse = create_pythia_70m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=SEQ_LEN,\n",
    "    use_sparse_attention=True,  # SPARSE\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nModel: Pythia-70M (Sparse Attention)\")\n",
    "print(f\"Parameters: ~{config_sparse.get_total_params() / 1e6:.1f}M\")\n",
    "print(f\"Sparse pattern: {config_sparse.sparse_pattern}\")\n",
    "print(f\"Block size: {config_sparse.sparse_block_size}\")\n",
    "\n",
    "# Train\n",
    "print(f\"\\nStarting training at {datetime.now().strftime('%H:%M:%S')}...\\n\")\n",
    "start_time_sparse = time.time()\n",
    "\n",
    "results_sparse = train(\n",
    "    model=model_sparse,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    config=config_sparse,\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seq_len=SEQ_LEN,\n",
    "    lr=LEARNING_RATE,\n",
    "    device=device,\n",
    "    checkpoint_dir='./checkpoints/sparse'\n",
    ")\n",
    "\n",
    "train_time_sparse = time.time() - start_time_sparse\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"SPARSE MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total training time: {train_time_sparse/60:.2f} minutes\")\n",
    "print(f\"Time per epoch: {train_time_sparse/EPOCHS:.2f} seconds\")\n",
    "print(f\"Final train loss: {results_sparse['train_losses'][-1]:.4f}\")\n",
    "print(f\"Final val loss: {results_sparse['val_losses'][-1]:.4f}\")\n",
    "print(f\"Best val loss: {results_sparse['best_val_loss']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING TIME COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "training_speedup = train_time_dense / train_time_sparse\n",
    "\n",
    "print(f\"\\nDense model:\")\n",
    "print(f\"  Total time: {train_time_dense/60:.2f} minutes\")\n",
    "print(f\"  Time per epoch: {train_time_dense/EPOCHS:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nSparse model:\")\n",
    "print(f\"  Total time: {train_time_sparse/60:.2f} minutes\")\n",
    "print(f\"  Time per epoch: {train_time_sparse/EPOCHS:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TRAINING SPEEDUP: {training_speedup:.2f}×\")\n",
    "print(f\"Time saved: {(train_time_dense - train_time_sparse)/60:.2f} minutes\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "models = ['Dense', 'Sparse']\n",
    "times = [train_time_dense/60, train_time_sparse/60]\n",
    "colors = ['steelblue', 'coral']\n",
    "\n",
    "bars = ax.bar(models, times, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, time_val) in enumerate(zip(bars, times)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{time_val:.2f} min',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Training Time (minutes)', fontsize=14, fontweight='bold')\n",
    "ax.set_title(f'Training Time Comparison ({EPOCHS} epochs)\\nSpeedup: {training_speedup:.2f}×', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/training_time_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loss Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING LOSS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Training loss comparison\n",
    "ax1.plot(epochs, results_dense['train_losses'], 'b-o', label='Dense', linewidth=2, markersize=6)\n",
    "ax1.plot(epochs, results_sparse['train_losses'], 'r-s', label='Sparse', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss comparison\n",
    "ax2.plot(epochs, results_dense['val_losses'], 'b-o', label='Dense', linewidth=2, markersize=6)\n",
    "ax2.plot(epochs, results_sparse['val_losses'], 'r-s', label='Sparse', linewidth=2, markersize=6)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax2.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss difference (Dense - Sparse)\n",
    "train_diff = [d - s for d, s in zip(results_dense['train_losses'], results_sparse['train_losses'])]\n",
    "val_diff = [d - s for d, s in zip(results_dense['val_losses'], results_sparse['val_losses'])]\n",
    "\n",
    "ax3.plot(epochs, train_diff, 'g-o', label='Training', linewidth=2, markersize=6)\n",
    "ax3.plot(epochs, val_diff, 'm-s', label='Validation', linewidth=2, markersize=6)\n",
    "ax3.axhline(y=0, color='k', linestyle='--', linewidth=1)\n",
    "ax3.set_xlabel('Epoch', fontsize=12)\n",
    "ax3.set_ylabel('Loss Difference (Dense - Sparse)', fontsize=12)\n",
    "ax3.set_title('Loss Gap Analysis', fontsize=14, fontweight='bold')\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Final metrics bar chart\n",
    "metrics = ['Train Loss', 'Val Loss', 'Best Val Loss']\n",
    "dense_metrics = [\n",
    "    results_dense['train_losses'][-1],\n",
    "    results_dense['val_losses'][-1],\n",
    "    results_dense['best_val_loss']\n",
    "]\n",
    "sparse_metrics = [\n",
    "    results_sparse['train_losses'][-1],\n",
    "    results_sparse['val_losses'][-1],\n",
    "    results_sparse['best_val_loss']\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, dense_metrics, width, label='Dense', color='steelblue', alpha=0.8)\n",
    "bars2 = ax4.bar(x + width/2, sparse_metrics, width, label='Sparse', color='coral', alpha=0.8)\n",
    "\n",
    "ax4.set_ylabel('Loss', fontsize=12)\n",
    "ax4.set_title('Final Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/loss_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nFinal Training Loss:\")\n",
    "print(f\"  Dense:  {results_dense['train_losses'][-1]:.4f}\")\n",
    "print(f\"  Sparse: {results_sparse['train_losses'][-1]:.4f}\")\n",
    "print(f\"  Difference: {abs(results_dense['train_losses'][-1] - results_sparse['train_losses'][-1]):.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Validation Loss:\")\n",
    "print(f\"  Dense:  {results_dense['val_losses'][-1]:.4f}\")\n",
    "print(f\"  Sparse: {results_sparse['val_losses'][-1]:.4f}\")\n",
    "print(f\"  Difference: {abs(results_dense['val_losses'][-1] - results_sparse['val_losses'][-1]):.4f}\")\n",
    "\n",
    "print(f\"\\nBest Validation Loss:\")\n",
    "print(f\"  Dense:  {results_dense['best_val_loss']:.4f}\")\n",
    "print(f\"  Sparse: {results_sparse['best_val_loss']:.4f}\")\n",
    "print(f\"  Difference: {abs(results_dense['best_val_loss'] - results_sparse['best_val_loss']):.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Speed Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE SPEED BENCHMARK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test configurations\n",
    "test_configs = [\n",
    "    {'batch': 4, 'seq': 64},\n",
    "    {'batch': 4, 'seq': 128},\n",
    "    {'batch': 4, 'seq': 256},\n",
    "    {'batch': 2, 'seq': 512},\n",
    "]\n",
    "\n",
    "inference_results = {\n",
    "    'configs': [],\n",
    "    'dense_times': [],\n",
    "    'sparse_times': [],\n",
    "    'speedups': []\n",
    "}\n",
    "\n",
    "model_dense.eval()\n",
    "model_sparse.eval()\n",
    "\n",
    "print(\"\\nRunning benchmarks...\\n\")\n",
    "\n",
    "for config in test_configs:\n",
    "    batch = config['batch']\n",
    "    seq = config['seq']\n",
    "    \n",
    "    print(f\"Configuration: Batch={batch}, SeqLen={seq}\")\n",
    "    \n",
    "    # Create input\n",
    "    input_ids = ndl.Tensor(\n",
    "        np.random.randint(0, vocab_size, (batch, seq)),\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Benchmark dense\n",
    "    times_dense = []\n",
    "    for i in range(10):  # 10 runs\n",
    "        start = time.time()\n",
    "        _, _ = model_dense(input_ids)\n",
    "        times_dense.append(time.time() - start)\n",
    "    \n",
    "    avg_dense = np.mean(times_dense[2:]) * 1000  # Exclude first 2, convert to ms\n",
    "    \n",
    "    # Benchmark sparse\n",
    "    times_sparse = []\n",
    "    for i in range(10):\n",
    "        start = time.time()\n",
    "        _, _ = model_sparse(input_ids)\n",
    "        times_sparse.append(time.time() - start)\n",
    "    \n",
    "    avg_sparse = np.mean(times_sparse[2:]) * 1000  # Exclude first 2, convert to ms\n",
    "    \n",
    "    speedup = avg_dense / avg_sparse\n",
    "    \n",
    "    print(f\"  Dense:   {avg_dense:.2f} ms\")\n",
    "    print(f\"  Sparse:  {avg_sparse:.2f} ms\")\n",
    "    print(f\"  Speedup: {speedup:.2f}×\\n\")\n",
    "    \n",
    "    inference_results['configs'].append(f\"B{batch}_S{seq}\")\n",
    "    inference_results['dense_times'].append(avg_dense)\n",
    "    inference_results['sparse_times'].append(avg_sparse)\n",
    "    inference_results['speedups'].append(speedup)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INFERENCE BENCHMARK COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Speed Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "labels = inference_results['configs']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "# Time comparison\n",
    "bars1 = ax1.bar(x - width/2, inference_results['dense_times'], width, \n",
    "                label='Dense', color='steelblue', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, inference_results['sparse_times'], width, \n",
    "                label='Sparse', color='coral', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Configuration', fontsize=12)\n",
    "ax1.set_ylabel('Forward Pass Time (ms)', fontsize=12)\n",
    "ax1.set_title('Inference Speed Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels, rotation=45)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Speedup\n",
    "bars = ax2.bar(x, inference_results['speedups'], color='green', alpha=0.7)\n",
    "ax2.axhline(y=1.0, color='r', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax2.set_xlabel('Configuration', fontsize=12)\n",
    "ax2.set_ylabel('Speedup (×)', fontsize=12)\n",
    "ax2.set_title('Sparse Attention Speedup', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(labels, rotation=45)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}×',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/inference_speed_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE SPEED SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Config':<12} {'Dense (ms)':<12} {'Sparse (ms)':<12} {'Speedup':<10}\")\n",
    "print(\"-\"*80)\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{label:<12} {inference_results['dense_times'][i]:<12.2f} \"\n",
    "          f\"{inference_results['sparse_times'][i]:<12.2f} \"\n",
    "          f\"{inference_results['speedups'][i]:<10.2f}×\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average speedup: {np.mean(inference_results['speedups']):.2f}×\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Text Generation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXT GENERATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create prompt\n",
    "prompt = ndl.Tensor(np.array([[1, 2, 3, 4, 5]]), device=device)\n",
    "\n",
    "print(f\"\\nPrompt tokens: {prompt.numpy()[0]}\")\n",
    "print(f\"Generating 20 tokens with each model...\\n\")\n",
    "\n",
    "# Generate with dense\n",
    "print(\"Dense Model:\")\n",
    "model_dense.eval()\n",
    "start = time.time()\n",
    "generated_dense = model_dense.generate(prompt, max_new_tokens=20, temperature=0.8)\n",
    "time_dense_gen = time.time() - start\n",
    "tokens_dense = generated_dense.numpy()[0].astype(int)\n",
    "print(f\"  Time: {time_dense_gen:.3f}s\")\n",
    "print(f\"  Generated: {tokens_dense}\")\n",
    "\n",
    "# Generate with sparse\n",
    "print(\"\\nSparse Model:\")\n",
    "model_sparse.eval()\n",
    "start = time.time()\n",
    "generated_sparse = model_sparse.generate(prompt, max_new_tokens=20, temperature=0.8)\n",
    "time_sparse_gen = time.time() - start\n",
    "tokens_sparse = generated_sparse.numpy()[0].astype(int)\n",
    "print(f\"  Time: {time_sparse_gen:.3f}s\")\n",
    "print(f\"  Generated: {tokens_sparse}\")\n",
    "\n",
    "gen_speedup = time_dense_gen / time_sparse_gen\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Generation speedup: {gen_speedup:.2f}×\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "models = ['Dense', 'Sparse']\n",
    "times = [time_dense_gen, time_sparse_gen]\n",
    "colors = ['steelblue', 'coral']\n",
    "\n",
    "bars = ax.bar(models, times, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar, time_val in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{time_val:.3f}s',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Generation Time (seconds)', fontsize=14, fontweight='bold')\n",
    "ax.set_title(f'Text Generation Speed (20 tokens)\\nSpeedup: {gen_speedup:.2f}×', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/generation_speed_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"#\" + \" \"*78 + \"#\")\n",
    "print(\"#\" + \" \"*20 + \"FINAL COMPARISON REPORT\" + \" \"*36 + \"#\")\n",
    "print(\"#\" + \" \"*78 + \"#\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Training tokens: {len(train_data):,}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Sequence length: {SEQ_LEN}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Time:\")\n",
    "print(f\"  Dense:   {train_time_dense/60:.2f} minutes\")\n",
    "print(f\"  Sparse:  {train_time_sparse/60:.2f} minutes\")\n",
    "print(f\"  Speedup: {training_speedup:.2f}×\")\n",
    "print(f\"  Time saved: {(train_time_dense - train_time_sparse)/60:.2f} minutes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL QUALITY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Training Loss:\")\n",
    "print(f\"  Dense:  {results_dense['train_losses'][-1]:.4f}\")\n",
    "print(f\"  Sparse: {results_sparse['train_losses'][-1]:.4f}\")\n",
    "print(f\"  Δ: {abs(results_dense['train_losses'][-1] - results_sparse['train_losses'][-1]):.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Validation Loss:\")\n",
    "print(f\"  Dense:  {results_dense['val_losses'][-1]:.4f}\")\n",
    "print(f\"  Sparse: {results_sparse['val_losses'][-1]:.4f}\")\n",
    "print(f\"  Δ: {abs(results_dense['val_losses'][-1] - results_sparse['val_losses'][-1]):.4f}\")\n",
    "\n",
    "print(f\"\\nBest Validation Loss:\")\n",
    "print(f\"  Dense:  {results_dense['best_val_loss']:.4f}\")\n",
    "print(f\"  Sparse: {results_sparse['best_val_loss']:.4f}\")\n",
    "print(f\"  Δ: {abs(results_dense['best_val_loss'] - results_sparse['best_val_loss']):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAverage Forward Pass Speedup: {np.mean(inference_results['speedups']):.2f}×\")\n",
    "print(f\"Min speedup: {min(inference_results['speedups']):.2f}×\")\n",
    "print(f\"Max speedup: {max(inference_results['speedups']):.2f}×\")\n",
    "\n",
    "print(f\"\\nText Generation Speedup: {gen_speedup:.2f}×\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "loss_diff = abs(results_dense['val_losses'][-1] - results_sparse['val_losses'][-1])\n",
    "\n",
    "print(f\"\\n✓ Training Speedup: {training_speedup:.2f}× faster with sparse attention\")\n",
    "print(f\"✓ Inference Speedup: {np.mean(inference_results['speedups']):.2f}× faster on average\")\n",
    "print(f\"✓ Model Quality: Loss difference of only {loss_diff:.4f} ({loss_diff/results_dense['val_losses'][-1]*100:.2f}%)\")\n",
    "print(f\"✓ Memory Efficiency: ~{75}% reduction in attention operations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSparse attention achieves {training_speedup:.2f}× training speedup and\")\n",
    "print(f\"{np.mean(inference_results['speedups']):.2f}× inference speedup while maintaining\")\n",
    "print(f\"comparable model quality (Δ loss < {loss_diff:.3f}).\")\n",
    "print(\"\\nThis makes sparse attention ideal for:\")\n",
    "print(\"  • Faster training iterations\")\n",
    "print(\"  • Real-time inference applications\")\n",
    "print(\"  • Longer sequence processing\")\n",
    "print(\"  • Resource-constrained environments\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"COMPARISON COMPLETE!\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "# Save summary to file\n",
    "with open('/content/comparison_summary.txt', 'w') as f:\n",
    "    f.write(\"DENSE VS SPARSE ATTENTION COMPARISON SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "    f.write(f\"Epochs: {EPOCHS}\\n\")\n",
    "    f.write(f\"Training speedup: {training_speedup:.2f}×\\n\")\n",
    "    f.write(f\"Inference speedup: {np.mean(inference_results['speedups']):.2f}×\\n\")\n",
    "    f.write(f\"Loss difference: {loss_diff:.4f}\\n\")\n",
    "    f.write(f\"\\nDense final val loss: {results_dense['val_losses'][-1]:.4f}\\n\")\n",
    "    f.write(f\"Sparse final val loss: {results_sparse['val_losses'][-1]:.4f}\\n\")\n",
    "\n",
    "print(\"\\n✓ Summary saved to /content/comparison_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
