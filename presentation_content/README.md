# Block-Sparse Attention: 90-Second Video Presentation

## Quick Start

1. **Open the presentation:**
   ```bash
   sparse_attention_presentation.html
   ```
   Double-click â†’ Opens in any browser

2. **Record 90 seconds:**
   - **Windows:** Win+G â†’ Xbox Game Bar â†’ Record
   - **Mac:** Cmd+Space â†’ QuickTime â†’ New Screen Recording
   - **Linux:** OBS Studio (free download)

3. **Upload to YouTube:**
   - Title: `Team 98 Block-Sparse Attention: Efficient Transformers`
   - âš ï¸ **DO NOT** check "made for kids"
   - Set to "Unlisted"
   - Copy link and submit

## What's Included

**Presentation:**
- Interactive HTML with all your benchmark results embedded
- Code snippets showing actual implementation
- CUDA kernel for block-sparse attention
- Needle framework sparse attention module
- Training script and model integration

**Code Sections in Presentation:**
- `src/ndarray_backend_cuda.cu` - CUDA kernel (~60 lines, full implementation)
- `python/needle/nn/nn_sparse_attention.py` - Sparse attention module
- `apps/pythia_model.py` - Pythia-70M model with sparse attention
- `apps/train_pythia.py` - Training loop

**Visualizations (10 images):**
- Inference speed (Pythia-70M & OPT-125M)
- Attention patterns comparison
- Memory usage analysis
- Training quality metrics
- Performance benchmarks

## What Gets Explained

**CUDA Kernel Section Shows:**
- CSR sparse format metadata structure
- Block-level parallelization
- Efficient computation of attention scores only for sparse positions
- Numerically stable softmax implementation
- Value aggregation using sparse weights

**Needle Module Section Shows:**
- Pattern generation (local, global, mixed)
- Multi-head attention with sparse masking
- Integration with standard transformer architecture

**Model Section Shows:**
- Configuration dataclass
- Token/positional embeddings
- Stacked sparse transformer layers
- Output projection and loss computation

**Training Section Shows:**
- Forward/backward pass
- Gradient computation and optimization
- Validation loop with perplexity tracking

## Timing (Perfect for 90 Seconds)

- 0-8s: Hero (title + key stats)
- 8-20s: Problem (dense attention complexity)
- 20-35s: Solution overview
- 35-50s: **CUDA kernel & Needle module code**
- 50-65s: **Model & training code**
- 65-80s: Performance results
- 80-90s: Conclusion

## File Structure

```
outputs/
â”œâ”€â”€ sparse_attention_presentation.html     (Main presentation)
â”œâ”€â”€ README.md                               (This file)
â”œâ”€â”€ attention_patterns.png                 (All your benchmark plots)
â”œâ”€â”€ inference_speed.png
â”œâ”€â”€ opt_inference_speed.png
â”œâ”€â”€ memory_comparison.png
â”œâ”€â”€ loss_perplexity_comparison.png
â”œâ”€â”€ opt_tinystories_results.png
â”œâ”€â”€ performance_comparison.png
â”œâ”€â”€ training_time.png
â””â”€â”€ attention_on_sentence.png
```

All files must stay in the same directory.

## Key Metrics Displayed

| Metric | Result |
|--------|--------|
| Inference Speedup | 2-4Ã— (1.9Ã— â†’ 6.8Ã—) |
| Memory Savings | 75% (134 MB â†’ 34 MB) |
| Quality Loss | < 0.1 (negligible) |
| Training Time | 1.27Ã— faster |

## Tips

- Scroll smoothly through presentation (natural pacing)
- Record at 1080p for best quality
- ~85-90 seconds is perfect
- Code sections are easily readable
- All images load automatically

## Troubleshooting

**Images not loading?**
â†’ Ensure all `.png` files in same folder as HTML

**Video too long/short?**
â†’ Adjust scroll speed while recording

**Can't upload to YouTube?**
â†’ Use Chrome, ensure you're logged in

---

**You're ready to record!** Your research is excellent, presentation is professional. Good luck! ğŸš€
