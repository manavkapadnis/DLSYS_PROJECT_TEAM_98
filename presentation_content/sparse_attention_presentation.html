<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Block-Sparse Attention: Implementation & Results</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #0f172a 100%);
            color: #e2e8f0;
            overflow-x: hidden;
        }
        html { scroll-behavior: smooth; }
        
        nav {
            position: fixed;
            top: 0;
            width: 100%;
            background: rgba(15, 23, 42, 0.95);
            backdrop-filter: blur(10px);
            padding: 1rem 2rem;
            z-index: 1000;
            border-bottom: 1px solid rgba(148, 163, 184, 0.1);
        }
        nav .logo {
            font-size: 1.5em;
            font-weight: bold;
            background: linear-gradient(45deg, #0ea5e9, #06b6d4);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        section {
            min-height: 100vh;
            padding: 80px 40px 40px;
            max-width: 1400px;
            margin: 0 auto;
            opacity: 0;
            animation: fadeInUp 0.8s ease-out forwards;
        }
        section:nth-child(1) { animation-delay: 0.1s; }
        section:nth-child(2) { animation-delay: 0.3s; }
        section:nth-child(3) { animation-delay: 0.5s; }
        section:nth-child(4) { animation-delay: 0.7s; }
        section:nth-child(5) { animation-delay: 0.9s; }
        section:nth-child(6) { animation-delay: 1.1s; }
        section:nth-child(7) { animation-delay: 1.3s; }
        section:nth-child(8) { animation-delay: 1.5s; }

        .hero {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            text-align: center;
            min-height: 100vh;
            background: linear-gradient(135deg, rgba(14, 165, 233, 0.1) 0%, rgba(6, 182, 212, 0.05) 100%);
            border-radius: 20px;
            padding: 60px 40px;
            margin: 20px 0;
        }
        .hero h1 {
            font-size: 4em;
            margin-bottom: 20px;
            background: linear-gradient(45deg, #0ea5e9, #06b6d4, #10b981);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            animation: slideDown 0.8s ease-out;
        }
        .hero p {
            font-size: 1.5em;
            color: #cbd5e1;
            margin-bottom: 30px;
            animation: slideUp 0.8s ease-out 0.2s both;
        }
        .hero .stats {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin-top: 40px;
            width: 100%;
            max-width: 800px;
        }
        .stat-box {
            background: rgba(30, 41, 59, 0.8);
            border: 2px solid rgba(14, 165, 233, 0.3);
            border-radius: 12px;
            padding: 25px;
            animation: popIn 0.6s ease-out;
            transition: all 0.3s ease;
        }
        .stat-box:hover {
            border-color: #0ea5e9;
            transform: translateY(-5px);
        }
        .stat-value {
            font-size: 2.5em;
            font-weight: bold;
            color: #10b981;
            margin-bottom: 10px;
        }
        .stat-label {
            font-size: 1em;
            color: #94a3b8;
        }

        h2 {
            font-size: 3em;
            margin-bottom: 40px;
            background: linear-gradient(45deg, #0ea5e9, #06b6d4);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            text-align: center;
        }
        h3 {
            font-size: 1.8em;
            color: #0ea5e9;
            margin: 30px 0 20px;
            border-left: 4px solid #0ea5e9;
            padding-left: 15px;
        }

        .content-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(500px, 1fr));
            gap: 30px;
            margin: 30px 0;
        }
        .content-item {
            background: rgba(30, 41, 59, 0.6);
            border: 1px solid rgba(148, 163, 184, 0.1);
            border-radius: 15px;
            padding: 25px;
            backdrop-filter: blur(10px);
            transition: all 0.3s ease;
        }
        .content-item:hover {
            border-color: rgba(14, 165, 233, 0.3);
            transform: translateY(-5px);
        }

        .image-container {
            width: 100%;
            background: rgba(15, 23, 42, 0.8);
            border-radius: 12px;
            padding: 20px;
            margin: 30px 0;
            display: flex;
            justify-content: center;
            border: 1px solid rgba(14, 165, 233, 0.2);
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }

        .image-label {
            text-align: center;
            color: #94a3b8;
            font-size: 0.95em;
            margin-top: 15px;
            font-style: italic;
        }

        .key-findings {
            background: linear-gradient(135deg, rgba(14, 165, 233, 0.1) 0%, rgba(10, 185, 135, 0.1) 100%);
            border-left: 4px solid #0ea5e9;
            border-radius: 8px;
            padding: 25px;
            margin: 30px 0;
        }
        .key-findings h4 {
            color: #0ea5e9;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        .finding-item {
            margin: 12px 0;
            padding-left: 25px;
            position: relative;
            color: #cbd5e1;
        }
        .finding-item::before {
            content: "‚úì";
            position: absolute;
            left: 0;
            color: #10b981;
            font-weight: bold;
        }

        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 25px;
            margin: 30px 0;
        }
        .comparison-card {
            background: rgba(30, 41, 59, 0.8);
            border-radius: 12px;
            padding: 30px;
            text-align: center;
            border: 2px solid rgba(148, 163, 184, 0.1);
        }
        .comparison-card:first-child {
            border-color: rgba(239, 68, 68, 0.3);
        }
        .comparison-card:last-child {
            border-color: rgba(16, 185, 129, 0.3);
        }
        .comparison-card h4 {
            font-size: 1.5em;
            margin-bottom: 20px;
        }
        .comparison-card:first-child h4 { color: #ef4444; }
        .comparison-card:last-child h4 { color: #10b981; }

        .metric {
            margin: 15px 0;
            padding: 12px;
            background: rgba(15, 23, 42, 0.6);
            border-radius: 8px;
        }
        .metric-label {
            color: #94a3b8;
            font-size: 0.9em;
            margin-bottom: 5px;
        }
        .metric-value {
            font-size: 1.5em;
            font-weight: bold;
        }
        .comparison-card:first-child .metric-value { color: #ef4444; }
        .comparison-card:last-child .metric-value { color: #10b981; }

        pre {
            background: rgba(15, 23, 42, 0.95);
            border: 1px solid rgba(14, 165, 233, 0.2);
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-size: 0.85em;
            line-height: 1.5;
        }
        code {
            font-family: 'Courier New', monospace;
            color: #0ea5e9;
        }

        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 30px;
            margin: 30px 0;
        }
        .result-card {
            background: rgba(30, 41, 59, 0.6);
            border-radius: 12px;
            overflow: hidden;
            border: 1px solid rgba(14, 165, 233, 0.2);
            transition: all 0.3s ease;
        }
        .result-card:hover {
            transform: translateY(-8px);
            border-color: rgba(14, 165, 233, 0.4);
        }
        .result-card-header {
            background: linear-gradient(135deg, rgba(14, 165, 233, 0.2), rgba(6, 182, 212, 0.2));
            padding: 20px;
        }
        .result-card-body {
            padding: 25px;
        }

        .highlight {
            background: linear-gradient(120deg, rgba(14, 165, 233, 0.2), rgba(10, 185, 135, 0.2));
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 3px solid #0ea5e9;
        }

        footer {
            background: rgba(15, 23, 42, 0.95);
            padding: 40px 20px;
            text-align: center;
            margin-top: 60px;
        }

        @keyframes fadeInUp {
            from { opacity: 0; transform: translateY(30px); }
            to { opacity: 1; transform: translateY(0); }
        }
        @keyframes slideDown {
            from { opacity: 0; transform: translateY(-30px); }
            to { opacity: 1; transform: translateY(0); }
        }
        @keyframes slideUp {
            from { opacity: 0; transform: translateY(30px); }
            to { opacity: 1; transform: translateY(0); }
        }
        @keyframes popIn {
            from { opacity: 0; transform: scale(0.8); }
            to { opacity: 1; transform: scale(1); }
        }

        @media (max-width: 768px) {
            section { padding: 80px 20px 40px; }
            .hero h1 { font-size: 2.5em; }
            .hero .stats { grid-template-columns: 1fr; }
            .comparison-grid { grid-template-columns: 1fr; }
            .content-grid { grid-template-columns: 1fr; }
            h2 { font-size: 2em; }
            pre { font-size: 0.75em; }
        }

        strong { color: #0ea5e9; }
    </style>
</head>
<body>
    <nav>
        <div class="logo">‚ö° Block-Sparse Attention - Implementation & Results</div>
    </nav>

    <!-- Hero -->
    <section class="hero">
        <h1>Block-Sparse Attention</h1>
        <p>Efficient Transformers with CUDA & Needle Framework</p>
        <div class="stats">
            <div class="stat-box">
                <div class="stat-value">2-4√ó</div>
                <div class="stat-label">Speedup</div>
            </div>
            <div class="stat-box">
                <div class="stat-value">75%</div>
                <div class="stat-label">Memory Saved</div>
            </div>
            <div class="stat-box">
                <div class="stat-value">&lt;0.1</div>
                <div class="stat-label">Loss Gap</div>
            </div>
        </div>
    </section>

    <!-- Problem -->
    <section>
        <h2>The Problem</h2>
        <div class="content-grid">
            <div class="content-item">
                <h3>Quadratic Complexity</h3>
                <p>Dense transformer attention has <strong>O(n¬≤)</strong> complexity. For 512 tokens: <strong>262K connections</strong></p>
                <p style="margin-top: 15px; color: #94a3b8;">Computationally prohibitive for long sequences.</p>
            </div>
            <div class="content-item">
                <h3>Massive Memory Usage</h3>
                <p>Attention matrices grow quadratically. OPT-125M at 1024 tokens: <strong>134.2 MB per layer</strong></p>
                <p style="margin-top: 15px; color: #94a3b8;">Limits sequence length on resource-constrained devices.</p>
            </div>
        </div>
    </section>

    <!-- Solution Overview -->
    <section>
        <h2>Our Solution: Block-Sparse Attention</h2>
        <p style="text-align: center; font-size: 1.2em; color: #cbd5e1; margin-bottom: 40px;">
            Reduce complexity from O(n¬≤) to O(n‚àön) with three configurable patterns
        </p>
        <div class="image-container">
            <img src="attention_patterns.png" alt="Attention Patterns">
        </div>
        <div class="image-label">Local (82% sparse), Global (68% sparse), Mixed (70% sparse)</div>
    </section>

    <!-- Code: CUDA Kernel -->
    <section>
        <h2>Technical Implementation</h2>
        
        <h3>üîß CUDA Kernel - Block Sparse Attention</h3>
        <p>GPU-optimized computation using CSR sparse format:</p>
        <pre><code>// src/ndarray_backend_cuda.cu
#include &lt;cuda_runtime.h&gt;

struct CSRMetadata {
    int* row_offsets;      // Start index per row
    int* col_indices;      // Column indices of non-zero blocks
    int nnz;               // Number of non-zero blocks
};

__global__ void BlockSparseAttentionKernel(
    float* Q, float* K, float* V, float* output,
    const CSRMetadata sparse_meta,
    int seq_len, int d_model, int num_heads, float scale) {
    
    int query_pos = threadIdx.x;
    extern __shared__ float shared_scores[];
    
    // Compute attention scores for SPARSE PATTERN ONLY
    for (int idx = sparse_meta.row_offsets[query_pos];
         idx &lt; sparse_meta.row_offsets[query_pos + 1]; idx++) {
        int key_pos = sparse_meta.col_indices[idx];
        float score = 0.0f;
        
        // Dot product Q¬∑K^T
        for (int d = threadIdx.y; d &lt; d_model; d += blockDim.y) {
            score += Q[query_pos * d_model + d] *
                     K[key_pos * d_model + d];
        }
        shared_scores[idx] = score * scale;
    }
    __syncthreads();
    
    // Numerically stable softmax (only sparse positions)
    float max_score = shared_scores[sparse_meta.row_offsets[query_pos]];
    for (int idx = sparse_meta.row_offsets[query_pos];
         idx &lt; sparse_meta.row_offsets[query_pos + 1]; idx++) {
        max_score = max(max_score, shared_scores[idx]);
    }
    
    float sum_exp = 0.0f;
    for (int idx = sparse_meta.row_offsets[query_pos];
         idx &lt; sparse_meta.row_offsets[query_pos + 1]; idx++) {
        float exp_score = expf(shared_scores[idx] - max_score);
        shared_scores[idx] = exp_score;
        sum_exp += exp_score;
    }
    
    // Output: Attention-weighted sum of values
    float attn_sum = 0.0f;
    for (int idx = sparse_meta.row_offsets[query_pos];
         idx &lt; sparse_meta.row_offsets[query_pos + 1]; idx++) {
        int key_pos = sparse_meta.col_indices[idx];
        float attn_weight = shared_scores[idx] / (sum_exp + 1e-9f);
        for (int d = threadIdx.y; d &lt; d_model; d += blockDim.y) {
            attn_sum += attn_weight * V[key_pos * d_model + d];
        }
    }
    output[query_pos * d_model + threadIdx.y] = attn_sum;
}</code></pre>

        <h3>üßµ Needle Sparse Attention Module</h3>
        <p>Python implementation with pattern generation:</p>
        <pre><code>// python/needle/nn/nn_sparse_attention.py
class BlockSparseMultiHeadAttention(ndl.nn.Module):
    def __init__(self, d_model, num_heads, sparse_pattern="mixed"):
        self.W_q = ndl.nn.Linear(d_model, d_model)
        self.W_k = ndl.nn.Linear(d_model, d_model)
        self.W_v = ndl.nn.Linear(d_model, d_model)
        self.W_o = ndl.nn.Linear(d_model, d_model)
        self.scale = 1.0 / np.sqrt(d_model // num_heads)
    
    def create_sparse_mask(self, seq_len):
        '''Generate attention pattern: local, global, or mixed'''
        if self.sparse_pattern == "local":
            # Sliding window: attend to nearby tokens only
            mask = np.zeros((seq_len, seq_len))
            for i in range(seq_len):
                start = max(0, i - 64)
                end = min(seq_len, i + 65)
                mask[i, start:end] = 1
            return mask
        elif self.sparse_pattern == "global":
            # Strided: attend to every k-th token globally
            mask = np.zeros((seq_len, seq_len))
            for i in range(seq_len):
                for j in range(0, seq_len, 2):
                    mask[i, j] = 1
                mask[i, i] = 1  # Self-attention
            return mask
        else:  # mixed
            # Combine local + global patterns
            local = self.create_sparse_mask_local(seq_len)
            global_pat = self.create_sparse_mask_global(seq_len)
            return np.maximum(local, global_pat)
    
    def forward(self, Q, K, V):
        # Project to multi-head
        Q = self.W_q(Q)
        K = self.W_k(K)
        V = self.W_v(V)
        
        # Compute attention: Q¬∑K^T / sqrt(d_k)
        scores = ndl.matmul(Q, K.transpose((-1, -2))) * self.scale
        
        # Apply sparse mask (non-sparse positions = -inf)
        sparse_mask = self.create_sparse_mask(Q.shape[1])
        scores = scores + (1 - sparse_mask) * (-1e9)
        
        # Softmax and output projection
        attn_weights = ndl.ops.softmax(scores, dim=-1)
        output = ndl.matmul(attn_weights, V)
        return self.W_o(output)</code></pre>

        <h3>ü§ñ Model Integration (Pythia-70M)</h3>
        <p>Complete model with sparse attention layers:</p>
        <pre><code>// apps/pythia_model.py
@dataclass
class PythiaConfig:
    vocab_size: int = 10000
    d_model: int = 512
    num_layers: int = 6
    num_heads: int = 8
    d_ff: int = 2048
    use_sparse_attention: bool = True
    sparse_pattern: str = "mixed"

class PythiaLM(ndl.nn.Module):
    def __init__(self, config):
        self.token_embed = ndl.nn.Embedding(config.vocab_size, config.d_model)
        self.pos_embed = ndl.nn.Embedding(config.max_seq_len, config.d_model)
        
        # Stack of sparse transformer layers
        self.transformer = ndl.nn.Sequential(*[
            SparseTransformerLayer(
                config.d_model, config.num_heads,
                config.d_ff, config.sparse_pattern)
            for _ in range(config.num_layers)
        ])
        
        self.lm_head = ndl.nn.Linear(config.d_model, config.vocab_size)
    
    def forward(self, input_ids, targets=None):
        # Token + positional embeddings
        x = self.token_embed(input_ids)
        x = x + self.pos_embed(ndl.arange(input_ids.shape[1]))
        
        # Apply sparse transformer layers
        x = self.transformer(x)
        logits = self.lm_head(x)
        
        # Compute loss if targets provided
        loss = None
        if targets is not None:
            loss = ndl.cross_entropy(logits, targets)
        
        return logits, loss</code></pre>

        <h3>üìö Training Loop</h3>
        <p>Standard training with sparse attention:</p>
        <pre><code>// apps/train_pythia.py
def train_epoch(model, train_loader, optimizer, device):
    for batch_idx, (input_ids, targets) in enumerate(train_loader):
        input_ids = ndl.Tensor(input_ids, device=device)
        targets = ndl.Tensor(targets, device=device)
        
        # Forward pass through sparse layers
        logits, loss = model(input_ids, targets)
        
        # Backward & optimize
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        if (batch_idx + 1) % 10 == 0:
            print(f"Loss: {loss.numpy():.4f}")

# Create and train model
model, config = create_pythia_70m(
    use_sparse_attention=True,
    sparse_pattern="mixed"
)
optimizer = Adam(model.parameters(), lr=3e-4)

for epoch in range(5):
    train_loss = train_epoch(model, train_loader, optimizer, device)
    val_loss, perplexity = evaluate(model, val_loader, device)
    print(f"Epoch {epoch+1}: Loss={val_loss:.4f}, PPL={perplexity:.1f}")</code></pre>
    </section>

    <!-- Results: Inference Speed -->
    <section>
        <h2>Performance Results</h2>
        
        <h3>‚ö° Inference Speed (Pythia-70M)</h3>
        <div class="image-container">
            <img src="inference_speed.png" alt="Inference Speed">
        </div>
        <div class="image-label">Forward pass speedup: 1.9√ó to 6.8√ó depending on sequence length</div>

        <h3 style="margin-top: 40px;">‚ö° Inference Speed (OPT-125M)</h3>
        <div class="image-container">
            <img src="opt_inference_speed.png" alt="OPT Inference">
        </div>
        <div class="image-label">Larger model shows even better improvements: 2.56√ó at 512 tokens</div>
    </section>

    <!-- Memory & Training -->
    <section>
        <h2>Memory Efficiency & Training</h2>
        
        <h3>üíæ Memory Usage</h3>
        <div class="image-container">
            <img src="memory_comparison.png" alt="Memory">
        </div>
        <div class="image-label">Attention memory per layer: 75% reduction at longer sequences</div>

        <div class="comparison-grid" style="margin-top: 40px;">
            <div class="comparison-card">
                <h4>Dense Attention</h4>
                <div class="metric">
                    <div class="metric-label">256 tokens:</div>
                    <div class="metric-value">33.6 MB</div>
                </div>
                <div class="metric">
                    <div class="metric-label">1024 tokens:</div>
                    <div class="metric-value">134.2 MB</div>
                </div>
            </div>
            <div class="comparison-card">
                <h4>Sparse Attention</h4>
                <div class="metric">
                    <div class="metric-label">256 tokens:</div>
                    <div class="metric-value">8.4 MB</div>
                </div>
                <div class="metric">
                    <div class="metric-label">1024 tokens:</div>
                    <div class="metric-value">33.6 MB</div>
                </div>
            </div>
        </div>

        <h3 style="margin-top: 40px;">üìà Training Quality</h3>
        <div class="image-container">
            <img src="loss_perplexity_comparison.png" alt="Training Loss">
        </div>
        <div class="image-label">Dense vs Sparse training curves: minimal quality gap, faster convergence</div>

        <h3 style="margin-top: 40px;">‚è±Ô∏è Training Speed</h3>
        <div class="image-container">
            <img src="training_time.png" alt="Training Time">
        </div>
        <div class="image-label">5 epochs at seq_len=128: 1.27√ó faster with sparse attention</div>
    </section>

    <!-- Results: Analysis -->
    <section>
        <h2>Comprehensive Analysis</h2>
        
        <div class="image-container">
            <img src="performance_comparison.png" alt="Performance">
        </div>
        <div class="image-label">Attention pattern performance: all sparse patterns outperform dense</div>

        <div class="image-container" style="margin-top: 40px;">
            <img src="opt_tinystories_results.png" alt="OPT TinyStories">
        </div>
        <div class="image-label">OPT-125M on TinyStories: quality fully preserved with 75% memory savings</div>

        <div class="key-findings" style="margin-top: 40px;">
            <h4>Key Results</h4>
            <div class="finding-item"><strong>Speed:</strong> 1.9√ó to 6.8√ó faster inference (Pythia)</div>
            <div class="finding-item"><strong>Memory:</strong> 75% reduction in attention matrices</div>
            <div class="finding-item"><strong>Quality:</strong> Loss difference &lt;0.1, perplexity within noise</div>
            <div class="finding-item"><strong>Training:</strong> 27% faster (8.7 min vs 11.1 min)</div>
        </div>
    </section>

    <!-- Conclusion -->
    <section>
        <h2>Impact & Conclusion</h2>
        
        <div class="results-grid">
            <div class="result-card">
                <div class="result-card-header">
                    <h3 style="color: #0ea5e9;">üöÄ Performance</h3>
                </div>
                <div class="result-card-body">
                    <p>2-4√ó inference speedup with proper sequence handling</p>
                    <p>Scaling improves dramatically with longer sequences</p>
                    <p>Practical speedups on both CPU and GPU</p>
                </div>
            </div>
            <div class="result-card">
                <div class="result-card-header">
                    <h3 style="color: #0ea5e9;">üíæ Efficiency</h3>
                </div>
                <div class="result-card-body">
                    <p>75% memory savings enables longer contexts</p>
                    <p>Reduces training/inference costs significantly</p>
                    <p>Practical for edge devices</p>
                </div>
            </div>
            <div class="result-card">
                <div class="result-card-header">
                    <h3 style="color: #0ea5e9;">‚úÖ Quality</h3>
                </div>
                <div class="result-card-body">
                    <p>Model expressiveness fully maintained</p>
                    <p>Loss difference negligible (&lt;0.1)</p>
                    <p>Perplexity within acceptable margins</p>
                </div>
            </div>
            <div class="result-card">
                <div class="result-card-header">
                    <h3 style="color: #0ea5e9;">üîß Implementation</h3>
                </div>
                <div class="result-card-body">
                    <p>Production-ready CUDA kernels</p>
                    <p>Seamless Needle framework integration</p>
                    <p>Drop-in replacement for standard attention</p>
                </div>
            </div>
        </div>

        <div class="highlight" style="margin-top: 40px; font-size: 1.1em; text-align: center;">
            <strong>Block-sparse attention: 2-4√ó speedup, 75% memory savings, quality preserved.</strong>
            <br><br>
            Production-ready implementation with CUDA kernels and Needle framework integration.
        </div>
    </section>

    <footer>
        <h3>Technologies</h3>
        <p style="color: #94a3b8; margin-top: 15px;">
            CUDA C++ ‚Ä¢ Needle Framework ‚Ä¢ Python ‚Ä¢ Block-Sparse Algorithms ‚Ä¢ Transformer Models
        </p>
        <p style="margin-top: 20px; font-size: 0.9em; color: #64748b;">
            Research implementation of efficient transformer inference and training
        </p>
    </footer>
</body>
</html>
