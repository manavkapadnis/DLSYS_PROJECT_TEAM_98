{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense vs Sparse Attention: Correctness and Performance\n",
    "\n",
    "This notebook verifies the implementation of Block-Sparse Attention and compares its performance against standard Dense Attention.\n",
    "\n",
    "## Goals\n",
    "1. **Verify Correctness**: Ensure the optimized CUDA/CPU kernels produce the same output as the reference implementation.\n",
    "2. **Benchmark Performance**: Measure the speedup of Sparse Attention for long sequences.\n",
    "3. **Visualize Sparsity**: Show the attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import needle as ndl\n",
    "import needle.nn as nn\n",
    "from needle.nn.nn_sparse_attention import BlockSparseMultiHeadAttention, BlockSparsePattern\n",
    "\n",
    "# Set device\n",
    "try:\n",
    "    device = ndl.cuda()\n",
    "    print(\"Using CUDA (GPU)\")\n",
    "except:\n",
    "    device = ndl.cpu()\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Needle backend: {ndl.backend_selection.BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Correctness Verification\n",
    "\n",
    "We compare the output of the optimized Sparse Attention kernel against a Dense Attention implementation that uses a mask to simulate sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_correctness(seq_len=128, block_size=16, num_heads=4, dim=32):\n",
    "    print(f\"Verifying with SeqLen={seq_len}, BlockSize={block_size}...\")\n",
    "    \n",
    "    # Create random inputs\n",
    "    q = ndl.Tensor(np.random.randn(1, num_heads, seq_len, dim), device=device)\n",
    "    k = ndl.Tensor(np.random.randn(1, num_heads, seq_len, dim), device=device)\n",
    "    v = ndl.Tensor(np.random.randn(1, num_heads, seq_len, dim), device=device)\n",
    "    \n",
    "    # Initialize Sparse Attention Module\n",
    "    sparse_attn = BlockSparseMultiHeadAttention(\n",
    "        device=device, \n",
    "        block_size=block_size, \n",
    "        sparse_pattern=\"local\",\n",
    "        dropout=0.0\n",
    "    )\n",
    "    \n",
    "    # 1. Run Optimized Sparse Attention (uses kernel if available)\n",
    "    # We force the use of the kernel by ensuring we are on CUDA/CPU and calling forward\n",
    "    out_sparse, _ = sparse_attn(q, k, v)\n",
    "    \n",
    "    # 2. Run Reference Implementation (Dense with Mask)\n",
    "    # We manually construct the mask and compute attention\n",
    "    # This mimics what the \"slow\" path in nn_sparse_attention.py does\n",
    "    \n",
    "    # Compute scores\n",
    "    scores = sparse_attn.matmul(q, k)\n",
    "    scores = scores / (dim ** 0.5)\n",
    "    \n",
    "    # Apply mask\n",
    "    mask = sparse_attn.create_block_mask(seq_len, device)\n",
    "    mask_tensor = ndl.Tensor(mask, device=device, requires_grad=False)\n",
    "    mask_expanded = mask_tensor.reshape((1, 1, seq_len, seq_len))\n",
    "    mask_broadcast = mask_expanded.broadcast_to(scores.shape)\n",
    "    \n",
    "    scores_masked = scores + mask_broadcast\n",
    "    \n",
    "    # Softmax\n",
    "    probs = sparse_attn.softmax(scores_masked)\n",
    "    \n",
    "    # Apply to values\n",
    "    v_transpose = ndl.ops.transpose(v, axes=(2, 3))\n",
    "    out_dense_masked = sparse_attn.matmul(probs, v_transpose)\n",
    "    \n",
    "    # Compare\n",
    "    diff = (out_sparse - out_dense_masked).numpy()\n",
    "    max_diff = np.abs(diff).max()\n",
    "    \n",
    "    print(f\"Max difference: {max_diff:.6f}\")\n",
    "    if max_diff < 1e-4:\n",
    "        print(\"✓ Verification PASSED\")\n",
    "    else:\n",
    "        print(\"✗ Verification FAILED\")\n",
    "        \n",
    "verify_correctness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Benchmark\n",
    "\n",
    "We measure the forward pass time for increasing sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(seq_lens=[128, 256, 512, 1024, 2048], block_size=32):\n",
    "    dense_times = []\n",
    "    sparse_times = []\n",
    "    \n",
    "    num_heads = 4\n",
    "    dim = 32\n",
    "    batch_size = 4\n",
    "    \n",
    "    print(f\"Benchmarking (Batch={batch_size}, Heads={num_heads}, Dim={dim})...\")\n",
    "    print(f\"{'SeqLen':<10} {'Dense (ms)':<15} {'Sparse (ms)':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    for seq_len in seq_lens:\n",
    "        # Inputs\n",
    "        q = ndl.Tensor(np.random.randn(batch_size, num_heads, seq_len, dim), device=device)\n",
    "        k = ndl.Tensor(np.random.randn(batch_size, num_heads, seq_len, dim), device=device)\n",
    "        v = ndl.Tensor(np.random.randn(batch_size, num_heads, seq_len, dim), device=device)\n",
    "        \n",
    "        # Sparse Module\n",
    "        sparse_attn = BlockSparseMultiHeadAttention(\n",
    "            device=device, block_size=block_size, sparse_pattern=\"local\"\n",
    "        )\n",
    "        \n",
    "        # Dense Reference (Standard Attention)\n",
    "        # We simulate dense by using a full mask (all ones)\n",
    "        # Or just use the matmul/softmax manually to avoid overhead of creating sparse mask\n",
    "        \n",
    "        def run_dense():\n",
    "            scores = sparse_attn.matmul(q, k) / (dim**0.5)\n",
    "            probs = sparse_attn.softmax(scores)\n",
    "            v_T = ndl.ops.transpose(v, axes=(2, 3))\n",
    "            return sparse_attn.matmul(probs, v_T)\n",
    "            \n",
    "        # Warmup\n",
    "        _ = run_dense()\n",
    "        _ = sparse_attn(q, k, v)\n",
    "        \n",
    "        # Time Dense\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            _ = run_dense()\n",
    "            if device.name == 'cuda':\n",
    "                ndl.backend_ndarray.ndarray_backend_cuda.Compact(q.compact()._handle, q.compact()._handle, [], [], 0) # Sync?\n",
    "                # Needle CUDA backend is synchronous for now usually, or we trust time.time()\n",
    "        dense_time = (time.time() - start) / 10 * 1000\n",
    "        \n",
    "        # Time Sparse\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            _ = sparse_attn(q, k, v)\n",
    "        sparse_time = (time.time() - start) / 10 * 1000\n",
    "        \n",
    "        speedup = dense_time / sparse_time\n",
    "        \n",
    "        dense_times.append(dense_time)\n",
    "        sparse_times.append(sparse_time)\n",
    "        \n",
    "        print(f\"{seq_len:<10} {dense_time:<15.2f} {sparse_time:<15.2f} {speedup:<10.2f}x\")\n",
    "        \n",
    "    return seq_lens, dense_times, sparse_times\n",
    "\n",
    "seq_lens, dense_times, sparse_times = benchmark_performance()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lens, dense_times, 'o-', label='Dense Attention')\n",
    "plt.plot(seq_lens, sparse_times, 's-', label='Sparse Attention')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Dense vs Sparse Attention Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sparsity Pattern Visualization\n",
    "\n",
    "Visualizing the block-sparse patterns used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pattern(pattern_name, seq_len=64, block_size=8):\n",
    "    if pattern_name == \"local\":\n",
    "        mask = BlockSparsePattern.local_pattern(seq_len, block_size)\n",
    "    elif pattern_name == \"global\":\n",
    "        mask = BlockSparsePattern.global_pattern(seq_len, block_size)\n",
    "    elif pattern_name == \"mixed\":\n",
    "        mask = BlockSparsePattern.mixed_pattern(seq_len, block_size)\n",
    "        \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(mask, cmap='binary', interpolation='nearest')\n",
    "    plt.title(f\"{pattern_name.capitalize()} Pattern (Blocks)\")\n",
    "    plt.xlabel(\"Key Blocks\")\n",
    "    plt.ylabel(\"Query Blocks\")\n",
    "    plt.grid(True, which='both', color='gray', linestyle='-', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "visualize_pattern(\"local\")\n",
    "visualize_pattern(\"global\")\n",
    "visualize_pattern(\"mixed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
