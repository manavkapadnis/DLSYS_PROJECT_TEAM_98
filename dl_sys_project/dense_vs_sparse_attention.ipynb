{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cdd33d",
   "metadata": {},
   "source": [
    "# Dense vs Sparse Attention: Correctness and Performance\n",
    "\n",
    "This notebook verifies the implementation of Block-Sparse Attention and compares its performance against standard Dense Attention.\n",
    "\n",
    "## Goals\n",
    "1. **Verify Correctness**: Ensure the optimized CUDA/CPU kernels produce the same output as the reference implementation.\n",
    "2. **Benchmark Performance**: Measure the speedup of Sparse Attention for long sequences.\n",
    "3. **Visualize Sparsity**: Show the attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a10225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking GPU availability...\n",
      "✓ GPU Available\n",
      "\n",
      "Rebuilding project...\n",
      "rm -rf build python/needle/backend_ndarray/ndarray_backend*.so\n",
      "-- The C compiler identification is GNU 13.3.0\n",
      "-- The CXX compiler identification is GNU 13.3.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Python: /usr/local/bin/python (found version \"3.12.3\") found components: Development Interpreter Development.Module Development.Embed \n",
      "-- Performing Test HAS_FLTO_AUTO\n",
      "-- Performing Test HAS_FLTO_AUTO - Success\n",
      "-- Found pybind11: /usr/local/lib/python3.12/dist-packages/pybind11/include (found version \"3.0.1\")\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Found CUDA: /usr/local/cuda (found version \"12.8\") \n",
      "-- Found cuda, building cuda backend\n",
      "Mon Nov 24 04:47:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   23C    P0             71W /  500W |     593MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.0\n",
      "-- Configuring done (3.5s)\n",
      "-- Generating done (0.2s)\n",
      "-- Build files have been written to: /workspace/manav/dl_sys_project/build\n",
      "make[1]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "make[2]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "make[3]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "make[3]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "make[3]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module /workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray_backend_cpu.cpython-312-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "[ 75%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
      "make[3]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "make[3]: Entering directory '/workspace/manav/dl_sys_project/build'\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module /workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray_backend_cuda.cpython-312-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "make[1]: Leaving directory '/workspace/manav/dl_sys_project/build'\n",
      "Using needle backend\n",
      "Warning: CUDA initialization failed: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'cuda_init'\n",
      "CUDA operations may not work correctly\n",
      "Using CUDA (GPU)\n",
      "Needle backend: nd\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Set project path - ADJUST THIS TO YOUR PATH\n",
    "project_path = '/workspace/manav/dl_sys_project/'\n",
    "os.chdir(project_path)\n",
    "\n",
    "# Check GPU\n",
    "print(\"Checking GPU availability...\")\n",
    "try:\n",
    "    import subprocess\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT)\n",
    "    print(\"✓ GPU Available\")\n",
    "except:\n",
    "    print(\"✗ No GPU detected - will use CPU\")\n",
    "\n",
    "# Rebuild project\n",
    "print(\"\\nRebuilding project...\")\n",
    "!make clean\n",
    "!make\n",
    "\n",
    "# Setup paths\n",
    "sys.path.insert(0, os.path.join(project_path, 'python'))\n",
    "sys.path.insert(0, os.path.join(project_path, 'apps'))\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import needle as ndl\n",
    "import needle.nn as nn\n",
    "from needle.nn.nn_sparse_attention import BlockSparseMultiHeadAttention, BlockSparsePattern\n",
    "\n",
    "# Set device\n",
    "try:\n",
    "    device = ndl.cuda()\n",
    "    print(\"Using CUDA (GPU)\")\n",
    "except:\n",
    "    device = ndl.cpu()\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Needle backend: {ndl.backend_selection.BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf21b8",
   "metadata": {},
   "source": [
    "## 1. Correctness Verification\n",
    "\n",
    "We compare the output of the optimized Sparse Attention kernel against a Dense Attention implementation that uses a mask to simulate sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f047b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying with SeqLen=128, BlockSize=16...\n",
      "Max difference: 0.000001\n",
      "✓ Verification PASSED\n"
     ]
    }
   ],
   "source": [
    "def verify_correctness(seq_len=128, block_size=16, num_heads=4, dim=32):\n",
    "    print(f\"Verifying with SeqLen={seq_len}, BlockSize={block_size}...\")\n",
    "    \n",
    "    # Create random inputs\n",
    "    q = ndl.Tensor(np.random.randn(1, num_heads, seq_len, dim), device=device)\n",
    "    k = ndl.Tensor(np.random.randn(1, num_heads, seq_len, dim), device=device)\n",
    "    v = ndl.Tensor(np.random.randn(1, num_heads, seq_len, dim), device=device)\n",
    "    \n",
    "    # Initialize Sparse Attention Module\n",
    "    sparse_attn = BlockSparseMultiHeadAttention(\n",
    "        device=device, \n",
    "        block_size=block_size, \n",
    "        sparse_pattern=\"local\",\n",
    "        dropout=0.0\n",
    "    )\n",
    "    \n",
    "    # 1. Run Optimized Sparse Attention (uses kernel if available)\n",
    "    # We force the use of the kernel by ensuring we are on CUDA/CPU and calling forward\n",
    "    out_sparse, _ = sparse_attn(q, k, v)\n",
    "    \n",
    "    # 2. Run Reference Implementation (Dense with Mask)\n",
    "    # We manually construct the mask and compute attention\n",
    "    # This mimics what the \"slow\" path in nn_sparse_attention.py does\n",
    "    \n",
    "    # Compute scores\n",
    "    scores = sparse_attn.matmul(q, k)\n",
    "    scores = scores / (dim ** 0.5)\n",
    "    \n",
    "    # Apply mask\n",
    "    mask = sparse_attn.create_block_mask(seq_len, device)\n",
    "    mask_tensor = ndl.Tensor(mask, device=device, requires_grad=False)\n",
    "    mask_expanded = mask_tensor.reshape((1, 1, seq_len, seq_len))\n",
    "    mask_broadcast = mask_expanded.broadcast_to(scores.shape)\n",
    "    \n",
    "    scores_masked = scores + mask_broadcast\n",
    "    \n",
    "    # Softmax\n",
    "    probs = sparse_attn.softmax(scores_masked)\n",
    "    \n",
    "    # Apply to values\n",
    "    v_transpose = ndl.ops.transpose(v, axes=(2, 3))\n",
    "    out_dense_masked = sparse_attn.matmul(probs, v_transpose)\n",
    "    \n",
    "    # Compare\n",
    "    diff = (out_sparse - out_dense_masked).numpy()\n",
    "    max_diff = np.abs(diff).max()\n",
    "    \n",
    "    print(f\"Max difference: {max_diff:.6f}\")\n",
    "    if max_diff < 1e-4:\n",
    "        print(\"✓ Verification PASSED\")\n",
    "    else:\n",
    "        print(\"✗ Verification FAILED\")\n",
    "        \n",
    "verify_correctness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a404cb",
   "metadata": {},
   "source": [
    "## 2. Performance Benchmark\n",
    "\n",
    "We measure the forward pass time for increasing sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92fcea42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking (Batch=4, Heads=4, Dim=32)...\n",
      "SeqLen     Dense (ms)      Sparse (ms)     Speedup   \n",
      "--------------------------------------------------\n",
      "Falling back to slow implementation: Kernel execution failed: an illegal memory access was encountered\n",
      "Attempting CUDA device reset to recover from error...\n",
      "CUDA reset failed: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'cuda_reset'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA Malloc failed: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     58\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_len\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<15.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<15.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeedup\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<10.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m seq_lens, dense_times, sparse_times\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m seq_lens, dense_times, sparse_times = \u001b[43mbenchmark_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Plot\u001b[39;00m\n\u001b[32m     65\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mbenchmark_performance\u001b[39m\u001b[34m(seq_lens, block_size)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Warmup\u001b[39;00m\n\u001b[32m     35\u001b[39m _ = run_dense()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m _ = \u001b[43msparse_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Time Dense\u001b[39;00m\n\u001b[32m     39\u001b[39m start = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_basic.py:74\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_sparse_attention.py:264\u001b[39m, in \u001b[36mBlockSparseMultiHeadAttention.forward\u001b[39m\u001b[34m(self, q, k, v)\u001b[39m\n\u001b[32m    261\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# Compute attention scores\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m scores = scores / (q_dim ** \u001b[32m0.5\u001b[39m)\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# Apply block-sparse mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/nn/nn_sparse_attention.py:205\u001b[39m, in \u001b[36mBlockSparseMultiHeadAttention.matmul\u001b[39m\u001b[34m(self, a, b_transpose)\u001b[39m\n\u001b[32m    202\u001b[39m broadcast_shape[-\u001b[32m3\u001b[39m] = a_shape[-\u001b[32m3\u001b[39m]\n\u001b[32m    203\u001b[39m b_transpose = b_transpose.broadcast_to(broadcast_shape)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_transpose\u001b[49m).sum(\u001b[38;5;28mlen\u001b[39m(a.shape) - \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:319\u001b[39m, in \u001b[36mTensor.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mneedle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEWiseMul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m needle.ops.MulScalar(other)(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:80\u001b[39m, in \u001b[36mTensorOp.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_from_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:242\u001b[39m, in \u001b[36mTensor.make_from_op\u001b[39m\u001b[34m(op, inputs)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor.requires_grad:\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor.detach()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealize_cached_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/autograd.py:107\u001b[39m, in \u001b[36mValue.realize_cached_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cached_data\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# note: data implicitly calls realized cached data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[38;5;28mself\u001b[39m.cached_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrealize_cached_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cached_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/ops/ops_mathematic.py:73\u001b[39m, in \u001b[36mEWiseMul.compute\u001b[39m\u001b[34m(self, a, b)\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m b.shape != broadcast_shape:\n\u001b[32m     71\u001b[39m         b = b.broadcast_to(broadcast_shape)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:556\u001b[39m, in \u001b[36mNDArray.__mul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: Union[\u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]) -> \u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mewise_or_scalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mewise_mul\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscalar_mul\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:534\u001b[39m, in \u001b[36mNDArray.ewise_or_scalar\u001b[39m\u001b[34m(self, other, ewise_func, scalar_func)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mewise_or_scalar\u001b[39m(\n\u001b[32m    526\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    527\u001b[39m     other: Union[\u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m],\n\u001b[32m    528\u001b[39m     ewise_func: Callable[[Any, Any, Any], \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m    529\u001b[39m     scalar_func: Callable[[Any, Any, Any], \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m    530\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mNDArray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    531\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;00m\n\u001b[32m    532\u001b[39m \u001b[33;03m    depending on whether \"other\" is an NDArray or scalar\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     out = \u001b[43mNDArray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, NDArray):\n\u001b[32m    536\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shape == other.shape, \u001b[33m\"\u001b[39m\u001b[33moperation needs two equal-sized arrays\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/manav/dl_sys_project/python/needle/backend_ndarray/ndarray.py:174\u001b[39m, in \u001b[36mNDArray.make\u001b[39m\u001b[34m(shape, strides, device, handle, offset)\u001b[39m\n\u001b[32m    172\u001b[39m array._device = device \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_device()\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     array._handle = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    176\u001b[39m     array._handle = handle\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA Malloc failed: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "def benchmark_performance(seq_lens=[128, 256, 512, 1024, 2048], block_size=32):\n",
    "    dense_times = []\n",
    "    sparse_times = []\n",
    "    \n",
    "    num_heads = 4\n",
    "    dim = 32\n",
    "    batch_size = 4\n",
    "    \n",
    "    print(f\"Benchmarking (Batch={batch_size}, Heads={num_heads}, Dim={dim})...\")\n",
    "    print(f\"{'SeqLen':<10} {'Dense (ms)':<15} {'Sparse (ms)':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    for seq_len in seq_lens:\n",
    "        # Inputs\n",
    "        q = ndl.Tensor(np.random.randn(batch_size, num_heads, seq_len, dim), device=device)\n",
    "        k = ndl.Tensor(np.random.randn(batch_size, num_heads, seq_len, dim), device=device)\n",
    "        v = ndl.Tensor(np.random.randn(batch_size, num_heads, seq_len, dim), device=device)\n",
    "        \n",
    "        # Sparse Module\n",
    "        sparse_attn = BlockSparseMultiHeadAttention(\n",
    "            device=device, block_size=block_size, sparse_pattern=\"local\"\n",
    "        )\n",
    "        \n",
    "        # Dense Reference (Standard Attention)\n",
    "        # We simulate dense by using a full mask (all ones)\n",
    "        # Or just use the matmul/softmax manually to avoid overhead of creating sparse mask\n",
    "        \n",
    "        def run_dense():\n",
    "            scores = sparse_attn.matmul(q, k) / (dim**0.5)\n",
    "            probs = sparse_attn.softmax(scores)\n",
    "            v_T = ndl.ops.transpose(v, axes=(2, 3))\n",
    "            return sparse_attn.matmul(probs, v_T)\n",
    "            \n",
    "        # Warmup\n",
    "        _ = run_dense()\n",
    "        _ = sparse_attn(q, k, v)\n",
    "        \n",
    "        # Time Dense\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            _ = run_dense()\n",
    "            if device.name == 'cuda':\n",
    "                ndl.backend_ndarray.ndarray_backend_cuda.Compact(q.compact()._handle, q.compact()._handle, [], [], 0) # Sync?\n",
    "                # Needle CUDA backend is synchronous for now usually, or we trust time.time()\n",
    "        dense_time = (time.time() - start) / 10 * 1000\n",
    "        \n",
    "        # Time Sparse\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            _ = sparse_attn(q, k, v)\n",
    "        sparse_time = (time.time() - start) / 10 * 1000\n",
    "        \n",
    "        speedup = dense_time / sparse_time\n",
    "        \n",
    "        dense_times.append(dense_time)\n",
    "        sparse_times.append(sparse_time)\n",
    "        \n",
    "        print(f\"{seq_len:<10} {dense_time:<15.2f} {sparse_time:<15.2f} {speedup:<10.2f}x\")\n",
    "        \n",
    "    return seq_lens, dense_times, sparse_times\n",
    "\n",
    "seq_lens, dense_times, sparse_times = benchmark_performance()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lens, dense_times, 'o-', label='Dense Attention')\n",
    "plt.plot(seq_lens, sparse_times, 's-', label='Sparse Attention')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Dense vs Sparse Attention Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311ad5b",
   "metadata": {},
   "source": [
    "## 3. Sparsity Pattern Visualization\n",
    "\n",
    "Visualizing the block-sparse patterns used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dae32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pattern(pattern_name, seq_len=64, block_size=8):\n",
    "    if pattern_name == \"local\":\n",
    "        mask = BlockSparsePattern.local_pattern(seq_len, block_size)\n",
    "    elif pattern_name == \"global\":\n",
    "        mask = BlockSparsePattern.global_pattern(seq_len, block_size)\n",
    "    elif pattern_name == \"mixed\":\n",
    "        mask = BlockSparsePattern.mixed_pattern(seq_len, block_size)\n",
    "        \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(mask, cmap='binary', interpolation='nearest')\n",
    "    plt.title(f\"{pattern_name.capitalize()} Pattern (Blocks)\")\n",
    "    plt.xlabel(\"Key Blocks\")\n",
    "    plt.ylabel(\"Query Blocks\")\n",
    "    plt.grid(True, which='both', color='gray', linestyle='-', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "visualize_pattern(\"local\")\n",
    "visualize_pattern(\"global\")\n",
    "visualize_pattern(\"mixed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
