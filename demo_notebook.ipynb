{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythia-70M with Block-Sparse Attention - Complete Demo\n",
    "\n",
    "## Deep Learning Systems Project\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Model architecture and creation\n",
    "2. Block-sparse attention patterns\n",
    "3. Training with HuggingFace datasets\n",
    "4. Performance benchmarking\n",
    "5. Model checkpointing and loading\n",
    "6. Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import needle as ndl\n",
    "import needle.nn as nn\n",
    "\n",
    "from pythia_model import create_pythia_70m, PythiaConfig\n",
    "from train_pythia import (\n",
    "    train, load_dataset_huggingface, batchify, \n",
    "    save_checkpoint, load_checkpoint\n",
    ")\n",
    "from needle.nn.nn_sparse_attention import BlockSparsePattern\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(f\"Needle backend: {ndl.backend_selection.BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = ndl.cpu()\n",
    "vocab_size = 10000\n",
    "max_seq_len = 256\n",
    "\n",
    "print(\"Creating Pythia-70M Models...\\n\")\n",
    "\n",
    "# Dense model\n",
    "print(\"1. Dense Attention Model\")\n",
    "model_dense, config_dense = create_pythia_70m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    use_sparse_attention=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Sparse model\n",
    "print(\"\\n2. Sparse Attention Model\")\n",
    "model_sparse, config_sparse = create_pythia_70m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    use_sparse_attention=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model Specifications:\")\n",
    "print(f\"  Total Parameters: ~{config_dense.get_total_params() / 1e6:.1f}M\")\n",
    "print(f\"  Layers: {config_dense.num_layers}\")\n",
    "print(f\"  Hidden Dimension: {config_dense.d_model}\")\n",
    "print(f\"  Attention Heads: {config_dense.num_heads}\")\n",
    "print(f\"  FFN Dimension: {config_dense.d_ff}\")\n",
    "print(f\"  Vocabulary Size: {vocab_size}\")\n",
    "print(f\"  Max Sequence Length: {max_seq_len}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Block-Sparse Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "block_size = 64\n",
    "\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Block size: {block_size}\")\n",
    "print(f\"Number of blocks: {seq_len // block_size}\\n\")\n",
    "\n",
    "# Generate patterns\n",
    "local_pattern = BlockSparsePattern.local_pattern(seq_len, block_size, window_size=1)\n",
    "global_pattern = BlockSparsePattern.global_pattern(seq_len, block_size, stride=2)\n",
    "mixed_pattern = BlockSparsePattern.mixed_pattern(seq_len, block_size, window_size=1, stride=4)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].imshow(local_pattern, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title('Local Pattern (Sliding Window)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Key Blocks')\n",
    "axes[0].set_ylabel('Query Blocks')\n",
    "axes[0].grid(False)\n",
    "\n",
    "axes[1].imshow(global_pattern, cmap='Greens', aspect='auto')\n",
    "axes[1].set_title('Global Pattern (Strided)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Key Blocks')\n",
    "axes[1].set_ylabel('Query Blocks')\n",
    "axes[1].grid(False)\n",
    "\n",
    "axes[2].imshow(mixed_pattern, cmap='Reds', aspect='auto')\n",
    "axes[2].set_title('Mixed Pattern (Local + Global)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Key Blocks')\n",
    "axes[2].set_ylabel('Query Blocks')\n",
    "axes[2].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/sparse_patterns_notebook.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate sparsity\n",
    "print(\"\\nSparsity Analysis:\")\n",
    "print(f\"  Local pattern: {(1 - local_pattern.sum() / local_pattern.size) * 100:.1f}% sparse\")\n",
    "print(f\"  Global pattern: {(1 - global_pattern.sum() / global_pattern.size) * 100:.1f}% sparse\")\n",
    "print(f\"  Mixed pattern: {(1 - mixed_pattern.sum() / mixed_pattern.size) * 100:.1f}% sparse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward Pass Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_len = 128\n",
    "\n",
    "# Create random input\n",
    "input_ids = ndl.Tensor(\n",
    "    np.random.randint(0, vocab_size, (batch_size, seq_len)),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dense forward pass\n",
    "print(\"Dense Attention Forward Pass...\")\n",
    "times_dense = []\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    logits_dense, _ = model_dense(input_ids)\n",
    "    elapsed = time.time() - start\n",
    "    times_dense.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.4f}s\")\n",
    "\n",
    "avg_time_dense = np.mean(times_dense[1:])\n",
    "print(f\"\\nAverage (excluding warmup): {avg_time_dense:.4f}s\")\n",
    "print(f\"Output shape: {logits_dense.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Sparse forward pass\n",
    "print(\"Sparse Attention Forward Pass...\")\n",
    "times_sparse = []\n",
    "for i in range(5):\n",
    "    start = time.time()\n",
    "    logits_sparse, _ = model_sparse(input_ids)\n",
    "    elapsed = time.time() - start\n",
    "    times_sparse.append(elapsed)\n",
    "    print(f\"  Run {i+1}: {elapsed:.4f}s\")\n",
    "\n",
    "avg_time_sparse = np.mean(times_sparse[1:])\n",
    "print(f\"\\nAverage (excluding warmup): {avg_time_sparse:.4f}s\")\n",
    "print(f\"Output shape: {logits_sparse.shape}\")\n",
    "\n",
    "speedup = avg_time_dense / avg_time_sparse\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Performance Summary:\")\n",
    "print(f\"  Dense: {avg_time_dense:.4f}s\")\n",
    "print(f\"  Sparse: {avg_time_sparse:.4f}s\")\n",
    "print(f\"  Speedup: {speedup:.2f}Ã—\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Loading (HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load WikiText-2\n",
    "print(\"Loading WikiText-2 dataset...\\n\")\n",
    "\n",
    "try:\n",
    "    train_data, val_data, vocab_size = load_dataset_huggingface(\n",
    "        \"wikitext-2\", \n",
    "        max_tokens=50000  # Small for demo\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Dataset loaded successfully!\")\n",
    "    print(f\"  Train tokens: {len(train_data):,}\")\n",
    "    print(f\"  Val tokens: {len(val_data):,}\")\n",
    "    print(f\"  Vocabulary size: {vocab_size:,}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample token indices: {train_data[:20]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Using synthetic data for demo...\")\n",
    "    vocab_size = 10000\n",
    "    train_data = np.random.randint(0, vocab_size, size=50000)\n",
    "    val_data = np.random.randint(0, vocab_size, size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Demo (Short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short training demo\n",
    "print(\"Running short training demo...\\n\")\n",
    "\n",
    "# Recreate smaller model for faster training\n",
    "model_demo, config_demo = create_pythia_70m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=64,  # Shorter for speed\n",
    "    use_sparse_attention=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train for just 2 epochs\n",
    "results = train(\n",
    "    model=model_demo,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    config=config_demo,\n",
    "    n_epochs=2,\n",
    "    batch_size=16,\n",
    "    seq_len=64,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    "    checkpoint_dir='./demo_checkpoints'\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training demo complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Demonstrating model checkpointing...\\n\")\n",
    "\n",
    "# Save checkpoint\n",
    "optimizer = ndl.optim.Adam(model_demo.parameters(), lr=1e-3)\n",
    "save_checkpoint(\n",
    "    model_demo, \n",
    "    optimizer, \n",
    "    epoch=2, \n",
    "    loss=results['train_losses'][-1],\n",
    "    filepath='/mnt/user-data/outputs/demo_checkpoint.pkl'\n",
    ")\n",
    "\n",
    "print(\"\\nLoading checkpoint back...\")\n",
    "loaded_model, loaded_opt, epoch, loss = load_checkpoint(\n",
    "    '/mnt/user-data/outputs/demo_checkpoint.pkl',\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Checkpoint save/load successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_losses' in results:\n",
    "    epochs = range(1, len(results['train_losses']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    ax1.plot(epochs, results['train_losses'], 'b-o', label='Training Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation loss\n",
    "    ax2.plot(epochs, results['val_losses'], 'r-s', label='Validation Loss', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Loss', fontsize=12)\n",
    "    ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/mnt/user-data/outputs/training_curves_notebook.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"  Final Train Loss: {results['train_losses'][-1]:.4f}\")\n",
    "    print(f\"  Final Val Loss: {results['val_losses'][-1]:.4f}\")\n",
    "else:\n",
    "    print(\"No training results available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [64, 128, 256, 512, 1024]\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "block_size = 64\n",
    "sparsity = 0.75\n",
    "\n",
    "dense_flops = []\n",
    "sparse_flops = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    # Dense: O(nÂ²d)\n",
    "    dense_ops = n_layers * seq_len * seq_len * d_model\n",
    "    # Sparse: O(n * block * d * (1 - sparsity))\n",
    "    sparse_ops = n_layers * seq_len * block_size * d_model * (1 - sparsity)\n",
    "    \n",
    "    dense_flops.append(dense_ops / 1e9)\n",
    "    sparse_flops.append(sparse_ops / 1e9)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(seq_lengths, dense_flops, 'b-o', label='Dense', linewidth=2, markersize=8)\n",
    "plt.plot(seq_lengths, sparse_flops, 'r-s', label='Sparse (75%)', linewidth=2, markersize=8)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('GFLOPs', fontsize=12)\n",
    "plt.title('Computational Complexity', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "speedups = [d/s for d, s in zip(dense_flops, sparse_flops)]\n",
    "plt.bar(range(len(seq_lengths)), speedups, color='green', alpha=0.7)\n",
    "plt.xticks(range(len(seq_lengths)), seq_lengths)\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', linewidth=2, label='Baseline')\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Theoretical Speedup (Ã—)', fontsize=12)\n",
    "plt.title('Sparse Attention Speedup', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(speedups):\n",
    "    plt.text(i, v + 0.1, f'{v:.2f}Ã—', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/mnt/user-data/outputs/complexity_analysis_notebook.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTheoretical Speedup:\")\n",
    "print(f\"{'Seq Len':<12} {'Dense GFLOPs':<15} {'Sparse GFLOPs':<15} {'Speedup':<10}\")\n",
    "print(\"-\"*60)\n",
    "for i, seq_len in enumerate(seq_lengths):\n",
    "    print(f\"{seq_len:<12} {dense_flops[i]:<15.2f} {sparse_flops[i]:<15.2f} {speedups[i]:<10.2f}Ã—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Text Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text Generation Demo\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create prompt\n",
    "prompt = ndl.Tensor(\n",
    "    np.array([[1, 2, 3, 4, 5]]),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Prompt tokens: {prompt.numpy()[0]}\")\n",
    "print(f\"\\nGenerating 15 tokens with sparse model...\")\n",
    "\n",
    "model_sparse.eval()\n",
    "generated = model_sparse.generate(\n",
    "    prompt, \n",
    "    max_new_tokens=15, \n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "generated_tokens = generated.numpy()[0]\n",
    "print(f\"Generated: {generated_tokens}\")\n",
    "\n",
    "print(\"\\nNote: This is a demonstration with an untrained model.\")\n",
    "print(\"For meaningful text, train the model on a real dataset first.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâœ“ Completed Demonstrations:\")\n",
    "print(\"  1. Model creation (dense and sparse)\")\n",
    "print(\"  2. Sparse attention pattern visualization\")\n",
    "print(\"  3. Forward pass performance comparison\")\n",
    "print(\"  4. HuggingFace dataset integration\")\n",
    "print(\"  5. Training pipeline\")\n",
    "print(\"  6. Model checkpointing and loading\")\n",
    "print(\"  7. Training curve visualization\")\n",
    "print(\"  8. Complexity analysis\")\n",
    "print(\"  9. Text generation\")\n",
    "\n",
    "print(\"\\nðŸ“Š Key Results:\")\n",
    "print(f\"  â€¢ Model Size: ~70M parameters\")\n",
    "print(f\"  â€¢ Forward Pass Speedup: {speedup:.2f}Ã—\")\n",
    "print(f\"  â€¢ Sparsity: ~75% for local pattern\")\n",
    "print(f\"  â€¢ Theoretical Speedup (1024 tokens): {speedups[-1]:.2f}Ã—\")\n",
    "\n",
    "print(\"\\nðŸš€ Next Steps:\")\n",
    "print(\"  1. Train on full WikiText-2 dataset\")\n",
    "print(\"  2. Run comprehensive benchmarks\")\n",
    "print(\"  3. Experiment with different sparse patterns\")\n",
    "print(\"  4. Scale to larger models\")\n",
    "print(\"  5. Optimize CUDA kernels\")\n",
    "\n",
    "print(\"\\nðŸ“š References:\")\n",
    "print(\"  â€¢ Pythia (Biderman et al., 2023)\")\n",
    "print(\"  â€¢ Sparse Transformer (Child et al., 2019)\")\n",
    "print(\"  â€¢ Longformer (Beltagy et al., 2020)\")\n",
    "print(\"  â€¢ BigBird (Zaheer et al., 2020)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEMO COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
  },
  "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
