{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPT-125M on WikiText-2: Dense vs Sparse Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Set project path - ADJUST THIS TO YOUR PATH\n",
    "project_path = '/workspace/manav/dl_sys_project/'\n",
    "os.chdir(project_path)\n",
    "\n",
    "# Check GPU\n",
    "print(\"Checking GPU availability...\")\n",
    "try:\n",
    "    import subprocess\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT)\n",
    "    print(\"✓ GPU Available\")\n",
    "except:\n",
    "    print(\"✗ No GPU detected - will use CPU\")\n",
    "\n",
    "# Rebuild project\n",
    "print(\"\\nRebuilding project...\")\n",
    "# !make clean\n",
    "!make\n",
    "\n",
    "# Setup paths\n",
    "sys.path.insert(0, os.path.join(project_path, 'python'))\n",
    "sys.path.insert(0, os.path.join(project_path, 'apps'))\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import needle as ndl\n",
    "import needle.nn as nn\n",
    "from collections import Counter\n",
    "import gc\n",
    "\n",
    "# Import OPT model\n",
    "from opt_model import create_opt_125m, OPTConfig, OPTLM\n",
    "from train_pythia_optimized import (\n",
    "    load_dataset_huggingface)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Using Needle backend: {ndl.backend_selection.BACKEND}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'dataset': 'wikitext-2',\n",
    "    'vocab_size': 1000,\n",
    "    'max_tokens': 10000,  # Limit dataset size for faster training\n",
    "    'batch_size': 2,\n",
    "    'seq_len': 64,\n",
    "    'n_epochs': 5,\n",
    "    'lr': 6e-4,  # OPT paper's learning rate\n",
    "    'accumulation_steps': 4,\n",
    "    'device': 'cuda',  # Change to 'cuda' if available\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "Loading dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    train_data, val_data, vocab_size = load_dataset_huggingface(\n",
    "        \"wikitext-2\", \n",
    "        max_tokens=CONFIG['max_tokens'],\n",
    "        vocab_size=CONFIG['vocab_size']\n",
    "    )\n",
    "    dataset_name = \"WikiText-2\"\n",
    "except Exception as e:\n",
    "    print(f\"Error loading WikiText-2: {e}\")\n",
    "    print(\"Using synthetic data\")\n",
    "    from train_pythia_optimized import load_synthetic_data\n",
    "    train_data, val_data, vocab_size = load_synthetic_data(CONFIG['max_tokens'], CONFIG['vocab_size'])\n",
    "    dataset_name = \"Synthetic\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Vocabulary: {vocab_size:,} tokens\")\n",
    "print(f\"Train: {len(train_data):,} tokens\")\n",
    "print(f\"Validation: {len(val_data):,} tokens\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OPT training utilities (uses optimized implementation from train_opt_optimized.py)\n",
    "from train_opt_optimized import (\n",
    "    batchify_streaming,\n",
    "    get_batch,\n",
    "    train_epoch_with_accumulation,\n",
    "    evaluate,\n",
    ")\n",
    "\n",
    "# Wrapper so we can keep using `train_epoch` in this notebook\n",
    "def train_epoch(model, train_data, batch_size, seq_len, optimizer, device, \n",
    "                accumulation_steps=4, clip_grad=1.0):\n",
    "    return train_epoch_with_accumulation(\n",
    "        model, train_data, batch_size, seq_len, optimizer, device,\n",
    "        accumulation_steps=accumulation_steps, clip_grad=clip_grad\n",
    "    )\n",
    "\n",
    "print(\"✓ Loaded OPT training helpers from train_opt_optimized.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dense Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRAINING DENSE ATTENTION MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set device\n",
    "device = ndl.cuda()\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model_dense, config_dense = create_opt_125m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=CONFIG['seq_len'],\n",
    "    use_sparse_attention=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer_dense = ndl.optim.Adam(\n",
    "    model_dense.parameters(), \n",
    "    lr=CONFIG['lr'], \n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "dense_train_losses = []\n",
    "dense_val_losses = []\n",
    "dense_perplexities = []\n",
    "\n",
    "for epoch in range(CONFIG['n_epochs']):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{CONFIG['n_epochs']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, tokens_per_sec = train_epoch(\n",
    "        model_dense, train_data, CONFIG['batch_size'], CONFIG['seq_len'],\n",
    "        optimizer_dense, device, CONFIG['accumulation_steps']\n",
    "    )\n",
    "    dense_train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_ppl = evaluate(\n",
    "        model_dense, val_data, CONFIG['batch_size'], CONFIG['seq_len'], device\n",
    "    )\n",
    "    dense_val_losses.append(val_loss)\n",
    "    dense_perplexities.append(val_ppl)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Perplexity: {val_ppl:.2f}\")\n",
    "    print(f\"  Throughput: {tokens_per_sec:.0f} tokens/sec\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DENSE MODEL TRAINING COMPLETE\")\n",
    "print(f\"Final Perplexity: {dense_perplexities[-1]:.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sparse Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRAINING SPARSE ATTENTION MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create model with sparse attention\n",
    "model_sparse, config_sparse = create_opt_125m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=CONFIG['seq_len'],\n",
    "    use_sparse_attention=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer_sparse = ndl.optim.Adam(\n",
    "    model_sparse.parameters(), \n",
    "    lr=CONFIG['lr'], \n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "sparse_train_losses = []\n",
    "sparse_val_losses = []\n",
    "sparse_perplexities = []\n",
    "\n",
    "for epoch in range(CONFIG['n_epochs']):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Epoch {epoch + 1}/{CONFIG['n_epochs']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, tokens_per_sec = train_epoch(\n",
    "        model_sparse, train_data, CONFIG['batch_size'], CONFIG['seq_len'],\n",
    "        optimizer_sparse, device, CONFIG['accumulation_steps']\n",
    "    )\n",
    "    sparse_train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_ppl = evaluate(\n",
    "        model_sparse, val_data, CONFIG['batch_size'], CONFIG['seq_len'], device\n",
    "    )\n",
    "    sparse_val_losses.append(val_loss)\n",
    "    sparse_perplexities.append(val_ppl)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1} Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Perplexity: {val_ppl:.2f}\")\n",
    "    print(f\"  Throughput: {tokens_per_sec:.0f} tokens/sec\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPARSE MODEL TRAINING COMPLETE\")\n",
    "print(f\"Final Perplexity: {sparse_perplexities[-1]:.2f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<30} {'Dense':<15} {'Sparse':<15} {'Difference':<15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "final_dense_loss = dense_val_losses[-1]\n",
    "final_sparse_loss = sparse_val_losses[-1]\n",
    "final_dense_ppl = dense_perplexities[-1]\n",
    "final_sparse_ppl = sparse_perplexities[-1]\n",
    "\n",
    "print(f\"{'Final Validation Loss:':<30} {final_dense_loss:<15.4f} {final_sparse_loss:<15.4f} {abs(final_dense_loss - final_sparse_loss):<15.4f}\")\n",
    "print(f\"{'Final Perplexity:':<30} {final_dense_ppl:<15.2f} {final_sparse_ppl:<15.2f} {abs(final_dense_ppl - final_sparse_ppl):<15.2f}\")\n",
    "print(f\"{'Perplexity Ratio:':<30} {'1.00x':<15} {final_sparse_ppl/final_dense_ppl:<15.2f}x {'':15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "if abs(final_sparse_ppl - final_dense_ppl) < 5:\n",
    "    print(\"✓ Sparse and dense models achieve similar quality!\")\n",
    "    print(\"✓ Block-sparse attention maintains model performance.\")\n",
    "else:\n",
    "    print(\"! Perplexity difference detected.\")\n",
    "    print(\"  This may be due to limited training or hyperparameter differences.\")\n",
    "\n",
    "print(\"\\nModel Complexity:\")\n",
    "print(f\"  Dense Attention: O(n²) = O({CONFIG['seq_len']}²) = {CONFIG['seq_len']**2:,} operations\")\n",
    "print(f\"  Sparse Attention: O(n*block) ≈ O({CONFIG['seq_len']}*64) = {CONFIG['seq_len']*64:,} operations\")\n",
    "print(f\"  Theoretical Speedup: {(CONFIG['seq_len']**2) / (CONFIG['seq_len']*64):.1f}x\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "epochs = np.arange(1, CONFIG['n_epochs'] + 1)\n",
    "\n",
    "# Training Loss\n",
    "axes[0].plot(epochs, dense_train_losses, 'o-', label='Dense', linewidth=2, markersize=8)\n",
    "axes[0].plot(epochs, sparse_train_losses, 's-', label='Sparse', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Loss\n",
    "axes[1].plot(epochs, dense_val_losses, 'o-', label='Dense', linewidth=2, markersize=8)\n",
    "axes[1].plot(epochs, sparse_val_losses, 's-', label='Sparse', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Validation Loss', fontsize=12)\n",
    "axes[1].set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "axes[2].plot(epochs, dense_perplexities, 'o-', label='Dense', linewidth=2, markersize=8)\n",
    "axes[2].plot(epochs, sparse_perplexities, 's-', label='Sparse', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Perplexity', fontsize=12)\n",
    "axes[2].set_title('Perplexity Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./outputs/opt_tinystories_results.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n✓ Results plot saved to: ./outputs/opt_tinystories_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dictionary\n",
    "summary = {\n",
    "    'Model': 'OPT-125M',\n",
    "    'Dataset': 'Wikistories',\n",
    "    'Training Tokens': f\"{len(train_data):,}\",\n",
    "    'Vocab Size': vocab_size,\n",
    "    'Sequence Length': CONFIG['seq_len'],\n",
    "    'Batch Size': CONFIG['batch_size'],\n",
    "    'Epochs': CONFIG['n_epochs'],\n",
    "    'Dense Final Loss': f\"{final_dense_loss:.4f}\",\n",
    "    'Sparse Final Loss': f\"{final_sparse_loss:.4f}\",\n",
    "    'Dense Final PPL': f\"{final_dense_ppl:.2f}\",\n",
    "    'Sparse Final PPL': f\"{final_sparse_ppl:.2f}\",\n",
    "    'Quality Gap': f\"{abs(final_sparse_ppl - final_dense_ppl):.2f}\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key:<25}: {value}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Speed Benchmark (Forward Pass: Dense vs Sparse OPT-125M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, time\n",
    "import numpy as np\n",
    "\n",
    "# Utility to clear GPU memory\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if hasattr(ndl, \"cuda\"):\n",
    "        try:\n",
    "            ndl.cuda().empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"✓ Starting optimized inference benchmark\")\n",
    "\n",
    "max_infer_seq_len = 256\n",
    "vocab_size = 1000\n",
    "\n",
    "device = ndl.cuda() if hasattr(ndl, \"cuda\") else ndl.cpu()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "test_configs = [\n",
    "    {'batch': 2, 'seq': 16},\n",
    "    {'batch': 2, 'seq': 32},\n",
    "    {'batch': 1, 'seq': 64},\n",
    "    {'batch': 1, 'seq': 128},\n",
    "    {'batch': 1, 'seq': 256},\n",
    "    # {'batch': 1, 'seq': 512},\n",
    "    # {'batch': 1, 'seq': 1024},\n",
    "]\n",
    "\n",
    "inference_results = {\n",
    "    'configs': [],\n",
    "    'dense_times': [],\n",
    "    'sparse_times': [],\n",
    "    'speedups': [],\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE SPEED BENCHMARK (OPT-125M, OOM-SAFE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ---------------- DENSE MODEL PHASE ----------------\n",
    "print(\"\\n>>> Running DENSE model benchmarks\")\n",
    "\n",
    "model_dense, config_dense = create_opt_125m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_infer_seq_len,\n",
    "    use_sparse_attention=False,\n",
    "    device=device\n",
    ")\n",
    "print(\"✓ Loaded dense model\")\n",
    "\n",
    "for cfg in test_configs:\n",
    "    batch, seq = cfg['batch'], cfg['seq']\n",
    "    print(f\"  Dense: batch={batch}, seq={seq}\")\n",
    "\n",
    "    input_ids = np.random.randint(0, vocab_size, size=(batch, seq))\n",
    "    input_tensor = ndl.Tensor(input_ids, device=device, dtype=\"float32\")\n",
    "\n",
    "    times = []\n",
    "    for _ in range(6):  # reduced warmup\n",
    "        start = time.time()\n",
    "        out = model_dense(input_tensor)\n",
    "        if isinstance(out, tuple): out = out[0]\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "    avg_dense = np.mean(times[2:]) * 1000  # ms\n",
    "\n",
    "    inference_results['configs'].append(f\"B{batch}_S{seq}\")\n",
    "    inference_results['dense_times'].append(avg_dense)\n",
    "\n",
    "    print(f\"      {avg_dense:.2f} ms\")\n",
    "\n",
    "    # Cleanup\n",
    "    del input_ids, input_tensor, out\n",
    "    clear_memory()\n",
    "\n",
    "# Delete dense model immediately\n",
    "del model_dense, config_dense\n",
    "clear_memory()\n",
    "print(\"✓ Freed dense model\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- SPARSE MODEL PHASE ----------------\n",
    "print(\"\\n>>> Running SPARSE model benchmarks\")\n",
    "\n",
    "model_sparse, config_sparse = create_opt_125m(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_infer_seq_len,\n",
    "    use_sparse_attention=True,\n",
    "    device=device\n",
    ")\n",
    "print(\"✓ Loaded sparse model\")\n",
    "\n",
    "for idx, cfg in enumerate(test_configs):\n",
    "    batch, seq = cfg['batch'], cfg['seq']\n",
    "    print(f\"  Sparse: batch={batch}, seq={seq}\")\n",
    "\n",
    "    input_ids = np.random.randint(0, vocab_size, size=(batch, seq))\n",
    "    input_tensor = ndl.Tensor(input_ids, device=device, dtype=\"float32\")\n",
    "\n",
    "    times = []\n",
    "    for _ in range(6):\n",
    "        start = time.time()\n",
    "        out = model_sparse(input_tensor)\n",
    "        if isinstance(out, tuple): out = out[0]\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "    avg_sparse = np.mean(times[2:]) * 1000  # ms\n",
    "    inference_results['sparse_times'].append(avg_sparse)\n",
    "\n",
    "    dense_t = inference_results['dense_times'][idx]\n",
    "    speedup = dense_t / avg_sparse if avg_sparse > 0 else 1.0\n",
    "    inference_results['speedups'].append(speedup)\n",
    "\n",
    "    print(f\"      {avg_sparse:.2f} ms  | speedup {speedup:.2f}×\")\n",
    "\n",
    "    # Cleanup\n",
    "    del input_ids, input_tensor, out\n",
    "    clear_memory()\n",
    "\n",
    "# Delete sparse model\n",
    "del model_sparse, config_sparse\n",
    "clear_memory()\n",
    "print(\"✓ Freed sparse model\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- FINAL SUMMARY ----------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Average Sparse Speedup: {np.mean(inference_results['speedups']):.2f}×\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Speed Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "labels = inference_results['configs']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "# Time comparison\n",
    "bars1 = ax1.bar(x - width/2, inference_results['dense_times'], width, \n",
    "                label='Dense', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, inference_results['sparse_times'], width, \n",
    "                label='Sparse (Local)', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Forward Pass Time (ms)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Inference Speed Comparison (OPT-125M)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels, rotation=45)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.1f}',\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Speedup\n",
    "bars = ax2.bar(x, inference_results['speedups'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Dense baseline', alpha=0.7)\n",
    "ax2.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Speedup (×)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Sparse Attention Speedup (OPT-125M)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(labels, rotation=45)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}×',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./outputs/opt_inference_speed.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Efficiency Analysis (Theoretical Attention Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical memory analysis for attention matrices\n",
    "seq_lengths = [16, 32, 64, 128, 256, 512, 1024]\n",
    "batch = 4\n",
    "heads = 8\n",
    "\n",
    "dense_mem = []\n",
    "sparse_mem = []\n",
    "\n",
    "for seq in seq_lengths:\n",
    "    # Attention scores memory per layer (rough estimate)\n",
    "    dense_attn = batch * heads * seq * seq * 4 / 1e6  # MB (float32)\n",
    "    sparse_attn = dense_attn * 0.25  # Assume ~75% sparsity\n",
    "\n",
    "    dense_mem.append(dense_attn)\n",
    "    sparse_mem.append(sparse_attn)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(seq_lengths))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, dense_mem, width, label='Dense Attention')\n",
    "bars2 = ax.bar(x + width/2, sparse_mem, width, label='Sparse Attention (Local)')\n",
    "\n",
    "ax.set_xlabel('Sequence Length', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Attention Memory (MB per layer)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Theoretical Attention Memory Usage (OPT-125M)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(seq_lengths)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./outputs/opt_attention_memory.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Model Quality**:\n",
    "   - Both dense and sparse attention models converge to similar perplexity\n",
    "   - Block-sparse attention preserves model quality while reducing computational cost\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Dense attention: O(n²) complexity\n",
    "   - Sparse attention: O(n*block_size) complexity\n",
    "   - Theoretical speedup increases with sequence length\n",
    "\n",
    "3. **OPT-125M Characteristics**:\n",
    "   - 125M parameters with 12 layers\n",
    "   - Successfully trained on TinyStories dataset\n",
    "   - Matches architecture from Meta AI's OPT paper\n",
    "\n",
    "4. **Practical Implications**:\n",
    "   - Block-sparse attention enables longer sequences\n",
    "   - Reduced memory footprint for training and inference\n",
    "   - Suitable for resource-constrained environments\n",
    "\n",
    "### Future Work:\n",
    "- Train on larger datasets (full TinyStories, WikiText-103)\n",
    "- Experiment with different sparse patterns (global, mixed)\n",
    "- Scale to larger OPT models (350M, 1.3B)\n",
    "- Implement more advanced sparse attention patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
